{
  "best_metric": 0.6766778230667114,
  "best_model_checkpoint": "./B/checkpoints/DistilBertUncased\\checkpoint-25000",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 50000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.08822648227214813,
      "learning_rate": 0.000998,
      "loss": 0.5146,
      "step": 500
    },
    {
      "epoch": 0.04,
      "grad_norm": 7.898756504058838,
      "learning_rate": 0.000996,
      "loss": 0.4484,
      "step": 1000
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.3587803840637207,
      "learning_rate": 0.000994,
      "loss": 0.4223,
      "step": 1500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.03922874853014946,
      "learning_rate": 0.000992,
      "loss": 0.4733,
      "step": 2000
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2478397935628891,
      "learning_rate": 0.00099,
      "loss": 0.4557,
      "step": 2500
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.367358207702637,
      "learning_rate": 0.000988,
      "loss": 0.4084,
      "step": 3000
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.264943927526474,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.4897,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.89871072769165,
      "learning_rate": 0.000984,
      "loss": 0.5166,
      "step": 4000
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2600928544998169,
      "learning_rate": 0.000982,
      "loss": 0.5035,
      "step": 4500
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.1040096282958984,
      "learning_rate": 0.00098,
      "loss": 0.4369,
      "step": 5000
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.964759349822998,
      "learning_rate": 0.000978,
      "loss": 0.4415,
      "step": 5500
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.2146785259246826,
      "learning_rate": 0.000976,
      "loss": 0.5569,
      "step": 6000
    },
    {
      "epoch": 0.26,
      "grad_norm": 34.042877197265625,
      "learning_rate": 0.000974,
      "loss": 0.4747,
      "step": 6500
    },
    {
      "epoch": 0.28,
      "grad_norm": 18.178424835205078,
      "learning_rate": 0.000972,
      "loss": 0.4769,
      "step": 7000
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.258935451507568,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.506,
      "step": 7500
    },
    {
      "epoch": 0.32,
      "grad_norm": 43.297447204589844,
      "learning_rate": 0.000968,
      "loss": 0.5244,
      "step": 8000
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.813616931438446,
      "learning_rate": 0.000966,
      "loss": 0.4894,
      "step": 8500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.9499895572662354,
      "learning_rate": 0.000964,
      "loss": 0.5357,
      "step": 9000
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09952063858509064,
      "learning_rate": 0.000962,
      "loss": 0.5248,
      "step": 9500
    },
    {
      "epoch": 0.4,
      "grad_norm": 25.061817169189453,
      "learning_rate": 0.00096,
      "loss": 0.5317,
      "step": 10000
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16409306228160858,
      "learning_rate": 0.000958,
      "loss": 0.4983,
      "step": 10500
    },
    {
      "epoch": 0.44,
      "grad_norm": 16.943370819091797,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.5527,
      "step": 11000
    },
    {
      "epoch": 0.46,
      "grad_norm": 53.3620491027832,
      "learning_rate": 0.000954,
      "loss": 0.519,
      "step": 11500
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.018714904785156,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.5125,
      "step": 12000
    },
    {
      "epoch": 0.5,
      "grad_norm": 22.545839309692383,
      "learning_rate": 0.00095,
      "loss": 0.528,
      "step": 12500
    },
    {
      "epoch": 0.52,
      "grad_norm": 59.90282440185547,
      "learning_rate": 0.000948,
      "loss": 0.5427,
      "step": 13000
    },
    {
      "epoch": 0.54,
      "grad_norm": 19.3067569732666,
      "learning_rate": 0.000946,
      "loss": 0.6068,
      "step": 13500
    },
    {
      "epoch": 0.56,
      "grad_norm": 49.90065383911133,
      "learning_rate": 0.000944,
      "loss": 0.5879,
      "step": 14000
    },
    {
      "epoch": 0.58,
      "grad_norm": 14.237058639526367,
      "learning_rate": 0.000942,
      "loss": 0.5435,
      "step": 14500
    },
    {
      "epoch": 0.6,
      "grad_norm": 147.92279052734375,
      "learning_rate": 0.00094,
      "loss": 0.5888,
      "step": 15000
    },
    {
      "epoch": 0.62,
      "grad_norm": 69.154541015625,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.5858,
      "step": 15500
    },
    {
      "epoch": 0.64,
      "grad_norm": 67.72359466552734,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.5002,
      "step": 16000
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.2266484498977661,
      "learning_rate": 0.000934,
      "loss": 0.5467,
      "step": 16500
    },
    {
      "epoch": 0.68,
      "grad_norm": 61.940452575683594,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.6364,
      "step": 17000
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.561913013458252,
      "learning_rate": 0.00093,
      "loss": 0.5913,
      "step": 17500
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.815735340118408,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5808,
      "step": 18000
    },
    {
      "epoch": 0.74,
      "grad_norm": 15.218993186950684,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.6192,
      "step": 18500
    },
    {
      "epoch": 0.76,
      "grad_norm": 50.52571487426758,
      "learning_rate": 0.000924,
      "loss": 0.5829,
      "step": 19000
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.213761329650879,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.6117,
      "step": 19500
    },
    {
      "epoch": 0.8,
      "grad_norm": 213.10328674316406,
      "learning_rate": 0.00092,
      "loss": 0.6177,
      "step": 20000
    },
    {
      "epoch": 0.82,
      "grad_norm": 77.04730224609375,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.6149,
      "step": 20500
    },
    {
      "epoch": 0.84,
      "grad_norm": 80.98167419433594,
      "learning_rate": 0.000916,
      "loss": 0.6679,
      "step": 21000
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.009888905100524426,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.7067,
      "step": 21500
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.15099625289440155,
      "learning_rate": 0.000912,
      "loss": 0.6685,
      "step": 22000
    },
    {
      "epoch": 0.9,
      "grad_norm": 61.906639099121094,
      "learning_rate": 0.00091,
      "loss": 0.6185,
      "step": 22500
    },
    {
      "epoch": 0.92,
      "grad_norm": 13.067138671875,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.6316,
      "step": 23000
    },
    {
      "epoch": 0.94,
      "grad_norm": 96.27442169189453,
      "learning_rate": 0.000906,
      "loss": 0.669,
      "step": 23500
    },
    {
      "epoch": 0.96,
      "grad_norm": 196.68402099609375,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.646,
      "step": 24000
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.008594074286520481,
      "learning_rate": 0.000902,
      "loss": 0.6281,
      "step": 24500
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.33812716603279114,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.7169,
      "step": 25000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": {
        "accuracy": 0.86475
      },
      "eval_loss": 0.6766778230667114,
      "eval_runtime": 39.8645,
      "eval_samples_per_second": 501.7,
      "eval_steps_per_second": 125.425,
      "step": 25000
    },
    {
      "epoch": 1.02,
      "grad_norm": 6.549378395080566,
      "learning_rate": 0.000898,
      "loss": 0.6699,
      "step": 25500
    },
    {
      "epoch": 1.04,
      "grad_norm": 41.62549591064453,
      "learning_rate": 0.000896,
      "loss": 0.6529,
      "step": 26000
    },
    {
      "epoch": 1.06,
      "grad_norm": 11.249669075012207,
      "learning_rate": 0.000894,
      "loss": 0.6429,
      "step": 26500
    },
    {
      "epoch": 1.08,
      "grad_norm": 153.14169311523438,
      "learning_rate": 0.000892,
      "loss": 0.7126,
      "step": 27000
    },
    {
      "epoch": 1.1,
      "grad_norm": 314.3110656738281,
      "learning_rate": 0.0008900000000000001,
      "loss": 0.6844,
      "step": 27500
    },
    {
      "epoch": 1.12,
      "grad_norm": 57.603904724121094,
      "learning_rate": 0.000888,
      "loss": 0.6797,
      "step": 28000
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.8486515283584595,
      "learning_rate": 0.0008860000000000001,
      "loss": 0.6694,
      "step": 28500
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4092085361480713,
      "learning_rate": 0.000884,
      "loss": 0.6365,
      "step": 29000
    },
    {
      "epoch": 1.18,
      "grad_norm": 245.41900634765625,
      "learning_rate": 0.000882,
      "loss": 0.6567,
      "step": 29500
    },
    {
      "epoch": 1.2,
      "grad_norm": 292.0746765136719,
      "learning_rate": 0.00088,
      "loss": 0.7074,
      "step": 30000
    },
    {
      "epoch": 1.22,
      "grad_norm": 6.3823442459106445,
      "learning_rate": 0.000878,
      "loss": 0.6259,
      "step": 30500
    },
    {
      "epoch": 1.24,
      "grad_norm": 68.0105972290039,
      "learning_rate": 0.000876,
      "loss": 0.6563,
      "step": 31000
    },
    {
      "epoch": 1.26,
      "grad_norm": 158.60580444335938,
      "learning_rate": 0.000874,
      "loss": 0.6663,
      "step": 31500
    },
    {
      "epoch": 1.28,
      "grad_norm": 13.43741226196289,
      "learning_rate": 0.000872,
      "loss": 0.7001,
      "step": 32000
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.012239448726177216,
      "learning_rate": 0.00087,
      "loss": 0.6646,
      "step": 32500
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.30624276399612427,
      "learning_rate": 0.0008680000000000001,
      "loss": 0.6669,
      "step": 33000
    },
    {
      "epoch": 1.34,
      "grad_norm": 360.9072570800781,
      "learning_rate": 0.000866,
      "loss": 0.6768,
      "step": 33500
    },
    {
      "epoch": 1.36,
      "grad_norm": 543.7313842773438,
      "learning_rate": 0.000864,
      "loss": 0.7024,
      "step": 34000
    },
    {
      "epoch": 1.38,
      "grad_norm": 116.29761505126953,
      "learning_rate": 0.000862,
      "loss": 0.6754,
      "step": 34500
    },
    {
      "epoch": 1.4,
      "grad_norm": 381.50762939453125,
      "learning_rate": 0.00086,
      "loss": 0.6955,
      "step": 35000
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.2527366578578949,
      "learning_rate": 0.000858,
      "loss": 0.6654,
      "step": 35500
    },
    {
      "epoch": 1.44,
      "grad_norm": 90.5650863647461,
      "learning_rate": 0.000856,
      "loss": 0.7017,
      "step": 36000
    },
    {
      "epoch": 1.46,
      "grad_norm": 363.1059265136719,
      "learning_rate": 0.000854,
      "loss": 0.791,
      "step": 36500
    },
    {
      "epoch": 1.48,
      "grad_norm": 3.192150354385376,
      "learning_rate": 0.000852,
      "loss": 0.7117,
      "step": 37000
    },
    {
      "epoch": 1.5,
      "grad_norm": 12.035703659057617,
      "learning_rate": 0.00085,
      "loss": 0.7058,
      "step": 37500
    },
    {
      "epoch": 1.52,
      "grad_norm": 21.619050979614258,
      "learning_rate": 0.000848,
      "loss": 0.8358,
      "step": 38000
    },
    {
      "epoch": 1.54,
      "grad_norm": 70.71847534179688,
      "learning_rate": 0.000846,
      "loss": 0.7002,
      "step": 38500
    },
    {
      "epoch": 1.56,
      "grad_norm": 111.39851379394531,
      "learning_rate": 0.000844,
      "loss": 0.7167,
      "step": 39000
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.13962626457214355,
      "learning_rate": 0.000842,
      "loss": 0.7172,
      "step": 39500
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.9017668962478638,
      "learning_rate": 0.00084,
      "loss": 0.7689,
      "step": 40000
    },
    {
      "epoch": 1.62,
      "grad_norm": 465.99920654296875,
      "learning_rate": 0.000838,
      "loss": 0.7167,
      "step": 40500
    },
    {
      "epoch": 1.64,
      "grad_norm": 369.94158935546875,
      "learning_rate": 0.0008359999999999999,
      "loss": 0.77,
      "step": 41000
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.16351844370365143,
      "learning_rate": 0.000834,
      "loss": 0.6791,
      "step": 41500
    },
    {
      "epoch": 1.68,
      "grad_norm": 421.8947448730469,
      "learning_rate": 0.000832,
      "loss": 0.7557,
      "step": 42000
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.7784960269927979,
      "learning_rate": 0.00083,
      "loss": 0.7602,
      "step": 42500
    },
    {
      "epoch": 1.72,
      "grad_norm": 7.238518714904785,
      "learning_rate": 0.000828,
      "loss": 0.7109,
      "step": 43000
    },
    {
      "epoch": 1.74,
      "grad_norm": 179.2086639404297,
      "learning_rate": 0.000826,
      "loss": 0.7203,
      "step": 43500
    },
    {
      "epoch": 1.76,
      "grad_norm": 236.70103454589844,
      "learning_rate": 0.000824,
      "loss": 0.7706,
      "step": 44000
    },
    {
      "epoch": 1.78,
      "grad_norm": 90.3520736694336,
      "learning_rate": 0.0008219999999999999,
      "loss": 0.7539,
      "step": 44500
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.042320728302002,
      "learning_rate": 0.00082,
      "loss": 0.7098,
      "step": 45000
    },
    {
      "epoch": 1.82,
      "grad_norm": 4.1188249588012695,
      "learning_rate": 0.0008179999999999999,
      "loss": 0.6973,
      "step": 45500
    },
    {
      "epoch": 1.84,
      "grad_norm": 1.163643717765808,
      "learning_rate": 0.000816,
      "loss": 0.72,
      "step": 46000
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.07819953560829163,
      "learning_rate": 0.0008139999999999999,
      "loss": 0.7134,
      "step": 46500
    },
    {
      "epoch": 1.88,
      "grad_norm": 587.955810546875,
      "learning_rate": 0.0008120000000000001,
      "loss": 0.7195,
      "step": 47000
    },
    {
      "epoch": 1.9,
      "grad_norm": 381.80517578125,
      "learning_rate": 0.0008100000000000001,
      "loss": 0.7537,
      "step": 47500
    },
    {
      "epoch": 1.92,
      "grad_norm": 144.209228515625,
      "learning_rate": 0.000808,
      "loss": 0.7228,
      "step": 48000
    },
    {
      "epoch": 1.94,
      "grad_norm": 86.45633697509766,
      "learning_rate": 0.0008060000000000001,
      "loss": 0.7481,
      "step": 48500
    },
    {
      "epoch": 1.96,
      "grad_norm": 106.03577423095703,
      "learning_rate": 0.000804,
      "loss": 0.7556,
      "step": 49000
    },
    {
      "epoch": 1.98,
      "grad_norm": 182.51702880859375,
      "learning_rate": 0.0008020000000000001,
      "loss": 0.7469,
      "step": 49500
    },
    {
      "epoch": 2.0,
      "grad_norm": 192.49668884277344,
      "learning_rate": 0.0008,
      "loss": 0.7631,
      "step": 50000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": {
        "accuracy": 0.85975
      },
      "eval_loss": 0.6808730363845825,
      "eval_runtime": 41.1541,
      "eval_samples_per_second": 485.979,
      "eval_steps_per_second": 121.495,
      "step": 50000
    }
  ],
  "logging_steps": 500,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 3748868756004288.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
