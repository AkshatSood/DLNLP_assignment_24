{
  "best_metric": 0.6766778230667114,
  "best_model_checkpoint": "./B/checkpoints/DistilBertUncased\\checkpoint-25000",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 75000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.08822648227214813,
      "learning_rate": 0.000998,
      "loss": 0.5146,
      "step": 500
    },
    {
      "epoch": 0.04,
      "grad_norm": 7.898756504058838,
      "learning_rate": 0.000996,
      "loss": 0.4484,
      "step": 1000
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.3587803840637207,
      "learning_rate": 0.000994,
      "loss": 0.4223,
      "step": 1500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.03922874853014946,
      "learning_rate": 0.000992,
      "loss": 0.4733,
      "step": 2000
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2478397935628891,
      "learning_rate": 0.00099,
      "loss": 0.4557,
      "step": 2500
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.367358207702637,
      "learning_rate": 0.000988,
      "loss": 0.4084,
      "step": 3000
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.264943927526474,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.4897,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.89871072769165,
      "learning_rate": 0.000984,
      "loss": 0.5166,
      "step": 4000
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2600928544998169,
      "learning_rate": 0.000982,
      "loss": 0.5035,
      "step": 4500
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.1040096282958984,
      "learning_rate": 0.00098,
      "loss": 0.4369,
      "step": 5000
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.964759349822998,
      "learning_rate": 0.000978,
      "loss": 0.4415,
      "step": 5500
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.2146785259246826,
      "learning_rate": 0.000976,
      "loss": 0.5569,
      "step": 6000
    },
    {
      "epoch": 0.26,
      "grad_norm": 34.042877197265625,
      "learning_rate": 0.000974,
      "loss": 0.4747,
      "step": 6500
    },
    {
      "epoch": 0.28,
      "grad_norm": 18.178424835205078,
      "learning_rate": 0.000972,
      "loss": 0.4769,
      "step": 7000
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.258935451507568,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.506,
      "step": 7500
    },
    {
      "epoch": 0.32,
      "grad_norm": 43.297447204589844,
      "learning_rate": 0.000968,
      "loss": 0.5244,
      "step": 8000
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.813616931438446,
      "learning_rate": 0.000966,
      "loss": 0.4894,
      "step": 8500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.9499895572662354,
      "learning_rate": 0.000964,
      "loss": 0.5357,
      "step": 9000
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09952063858509064,
      "learning_rate": 0.000962,
      "loss": 0.5248,
      "step": 9500
    },
    {
      "epoch": 0.4,
      "grad_norm": 25.061817169189453,
      "learning_rate": 0.00096,
      "loss": 0.5317,
      "step": 10000
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16409306228160858,
      "learning_rate": 0.000958,
      "loss": 0.4983,
      "step": 10500
    },
    {
      "epoch": 0.44,
      "grad_norm": 16.943370819091797,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.5527,
      "step": 11000
    },
    {
      "epoch": 0.46,
      "grad_norm": 53.3620491027832,
      "learning_rate": 0.000954,
      "loss": 0.519,
      "step": 11500
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.018714904785156,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.5125,
      "step": 12000
    },
    {
      "epoch": 0.5,
      "grad_norm": 22.545839309692383,
      "learning_rate": 0.00095,
      "loss": 0.528,
      "step": 12500
    },
    {
      "epoch": 0.52,
      "grad_norm": 59.90282440185547,
      "learning_rate": 0.000948,
      "loss": 0.5427,
      "step": 13000
    },
    {
      "epoch": 0.54,
      "grad_norm": 19.3067569732666,
      "learning_rate": 0.000946,
      "loss": 0.6068,
      "step": 13500
    },
    {
      "epoch": 0.56,
      "grad_norm": 49.90065383911133,
      "learning_rate": 0.000944,
      "loss": 0.5879,
      "step": 14000
    },
    {
      "epoch": 0.58,
      "grad_norm": 14.237058639526367,
      "learning_rate": 0.000942,
      "loss": 0.5435,
      "step": 14500
    },
    {
      "epoch": 0.6,
      "grad_norm": 147.92279052734375,
      "learning_rate": 0.00094,
      "loss": 0.5888,
      "step": 15000
    },
    {
      "epoch": 0.62,
      "grad_norm": 69.154541015625,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.5858,
      "step": 15500
    },
    {
      "epoch": 0.64,
      "grad_norm": 67.72359466552734,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.5002,
      "step": 16000
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.2266484498977661,
      "learning_rate": 0.000934,
      "loss": 0.5467,
      "step": 16500
    },
    {
      "epoch": 0.68,
      "grad_norm": 61.940452575683594,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.6364,
      "step": 17000
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.561913013458252,
      "learning_rate": 0.00093,
      "loss": 0.5913,
      "step": 17500
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.815735340118408,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5808,
      "step": 18000
    },
    {
      "epoch": 0.74,
      "grad_norm": 15.218993186950684,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.6192,
      "step": 18500
    },
    {
      "epoch": 0.76,
      "grad_norm": 50.52571487426758,
      "learning_rate": 0.000924,
      "loss": 0.5829,
      "step": 19000
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.213761329650879,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.6117,
      "step": 19500
    },
    {
      "epoch": 0.8,
      "grad_norm": 213.10328674316406,
      "learning_rate": 0.00092,
      "loss": 0.6177,
      "step": 20000
    },
    {
      "epoch": 0.82,
      "grad_norm": 77.04730224609375,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.6149,
      "step": 20500
    },
    {
      "epoch": 0.84,
      "grad_norm": 80.98167419433594,
      "learning_rate": 0.000916,
      "loss": 0.6679,
      "step": 21000
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.009888905100524426,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.7067,
      "step": 21500
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.15099625289440155,
      "learning_rate": 0.000912,
      "loss": 0.6685,
      "step": 22000
    },
    {
      "epoch": 0.9,
      "grad_norm": 61.906639099121094,
      "learning_rate": 0.00091,
      "loss": 0.6185,
      "step": 22500
    },
    {
      "epoch": 0.92,
      "grad_norm": 13.067138671875,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.6316,
      "step": 23000
    },
    {
      "epoch": 0.94,
      "grad_norm": 96.27442169189453,
      "learning_rate": 0.000906,
      "loss": 0.669,
      "step": 23500
    },
    {
      "epoch": 0.96,
      "grad_norm": 196.68402099609375,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.646,
      "step": 24000
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.008594074286520481,
      "learning_rate": 0.000902,
      "loss": 0.6281,
      "step": 24500
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.33812716603279114,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.7169,
      "step": 25000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": {
        "accuracy": 0.86475
      },
      "eval_loss": 0.6766778230667114,
      "eval_runtime": 39.8645,
      "eval_samples_per_second": 501.7,
      "eval_steps_per_second": 125.425,
      "step": 25000
    },
    {
      "epoch": 1.02,
      "grad_norm": 6.549378395080566,
      "learning_rate": 0.000898,
      "loss": 0.6699,
      "step": 25500
    },
    {
      "epoch": 1.04,
      "grad_norm": 41.62549591064453,
      "learning_rate": 0.000896,
      "loss": 0.6529,
      "step": 26000
    },
    {
      "epoch": 1.06,
      "grad_norm": 11.249669075012207,
      "learning_rate": 0.000894,
      "loss": 0.6429,
      "step": 26500
    },
    {
      "epoch": 1.08,
      "grad_norm": 153.14169311523438,
      "learning_rate": 0.000892,
      "loss": 0.7126,
      "step": 27000
    },
    {
      "epoch": 1.1,
      "grad_norm": 314.3110656738281,
      "learning_rate": 0.0008900000000000001,
      "loss": 0.6844,
      "step": 27500
    },
    {
      "epoch": 1.12,
      "grad_norm": 57.603904724121094,
      "learning_rate": 0.000888,
      "loss": 0.6797,
      "step": 28000
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.8486515283584595,
      "learning_rate": 0.0008860000000000001,
      "loss": 0.6694,
      "step": 28500
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4092085361480713,
      "learning_rate": 0.000884,
      "loss": 0.6365,
      "step": 29000
    },
    {
      "epoch": 1.18,
      "grad_norm": 245.41900634765625,
      "learning_rate": 0.000882,
      "loss": 0.6567,
      "step": 29500
    },
    {
      "epoch": 1.2,
      "grad_norm": 292.0746765136719,
      "learning_rate": 0.00088,
      "loss": 0.7074,
      "step": 30000
    },
    {
      "epoch": 1.22,
      "grad_norm": 6.3823442459106445,
      "learning_rate": 0.000878,
      "loss": 0.6259,
      "step": 30500
    },
    {
      "epoch": 1.24,
      "grad_norm": 68.0105972290039,
      "learning_rate": 0.000876,
      "loss": 0.6563,
      "step": 31000
    },
    {
      "epoch": 1.26,
      "grad_norm": 158.60580444335938,
      "learning_rate": 0.000874,
      "loss": 0.6663,
      "step": 31500
    },
    {
      "epoch": 1.28,
      "grad_norm": 13.43741226196289,
      "learning_rate": 0.000872,
      "loss": 0.7001,
      "step": 32000
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.012239448726177216,
      "learning_rate": 0.00087,
      "loss": 0.6646,
      "step": 32500
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.30624276399612427,
      "learning_rate": 0.0008680000000000001,
      "loss": 0.6669,
      "step": 33000
    },
    {
      "epoch": 1.34,
      "grad_norm": 360.9072570800781,
      "learning_rate": 0.000866,
      "loss": 0.6768,
      "step": 33500
    },
    {
      "epoch": 1.36,
      "grad_norm": 543.7313842773438,
      "learning_rate": 0.000864,
      "loss": 0.7024,
      "step": 34000
    },
    {
      "epoch": 1.38,
      "grad_norm": 116.29761505126953,
      "learning_rate": 0.000862,
      "loss": 0.6754,
      "step": 34500
    },
    {
      "epoch": 1.4,
      "grad_norm": 381.50762939453125,
      "learning_rate": 0.00086,
      "loss": 0.6955,
      "step": 35000
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.2527366578578949,
      "learning_rate": 0.000858,
      "loss": 0.6654,
      "step": 35500
    },
    {
      "epoch": 1.44,
      "grad_norm": 90.5650863647461,
      "learning_rate": 0.000856,
      "loss": 0.7017,
      "step": 36000
    },
    {
      "epoch": 1.46,
      "grad_norm": 363.1059265136719,
      "learning_rate": 0.000854,
      "loss": 0.791,
      "step": 36500
    },
    {
      "epoch": 1.48,
      "grad_norm": 3.192150354385376,
      "learning_rate": 0.000852,
      "loss": 0.7117,
      "step": 37000
    },
    {
      "epoch": 1.5,
      "grad_norm": 12.035703659057617,
      "learning_rate": 0.00085,
      "loss": 0.7058,
      "step": 37500
    },
    {
      "epoch": 1.52,
      "grad_norm": 21.619050979614258,
      "learning_rate": 0.000848,
      "loss": 0.8358,
      "step": 38000
    },
    {
      "epoch": 1.54,
      "grad_norm": 70.71847534179688,
      "learning_rate": 0.000846,
      "loss": 0.7002,
      "step": 38500
    },
    {
      "epoch": 1.56,
      "grad_norm": 111.39851379394531,
      "learning_rate": 0.000844,
      "loss": 0.7167,
      "step": 39000
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.13962626457214355,
      "learning_rate": 0.000842,
      "loss": 0.7172,
      "step": 39500
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.9017668962478638,
      "learning_rate": 0.00084,
      "loss": 0.7689,
      "step": 40000
    },
    {
      "epoch": 1.62,
      "grad_norm": 465.99920654296875,
      "learning_rate": 0.000838,
      "loss": 0.7167,
      "step": 40500
    },
    {
      "epoch": 1.64,
      "grad_norm": 369.94158935546875,
      "learning_rate": 0.0008359999999999999,
      "loss": 0.77,
      "step": 41000
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.16351844370365143,
      "learning_rate": 0.000834,
      "loss": 0.6791,
      "step": 41500
    },
    {
      "epoch": 1.68,
      "grad_norm": 421.8947448730469,
      "learning_rate": 0.000832,
      "loss": 0.7557,
      "step": 42000
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.7784960269927979,
      "learning_rate": 0.00083,
      "loss": 0.7602,
      "step": 42500
    },
    {
      "epoch": 1.72,
      "grad_norm": 7.238518714904785,
      "learning_rate": 0.000828,
      "loss": 0.7109,
      "step": 43000
    },
    {
      "epoch": 1.74,
      "grad_norm": 179.2086639404297,
      "learning_rate": 0.000826,
      "loss": 0.7203,
      "step": 43500
    },
    {
      "epoch": 1.76,
      "grad_norm": 236.70103454589844,
      "learning_rate": 0.000824,
      "loss": 0.7706,
      "step": 44000
    },
    {
      "epoch": 1.78,
      "grad_norm": 90.3520736694336,
      "learning_rate": 0.0008219999999999999,
      "loss": 0.7539,
      "step": 44500
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.042320728302002,
      "learning_rate": 0.00082,
      "loss": 0.7098,
      "step": 45000
    },
    {
      "epoch": 1.82,
      "grad_norm": 4.1188249588012695,
      "learning_rate": 0.0008179999999999999,
      "loss": 0.6973,
      "step": 45500
    },
    {
      "epoch": 1.84,
      "grad_norm": 1.163643717765808,
      "learning_rate": 0.000816,
      "loss": 0.72,
      "step": 46000
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.07819953560829163,
      "learning_rate": 0.0008139999999999999,
      "loss": 0.7134,
      "step": 46500
    },
    {
      "epoch": 1.88,
      "grad_norm": 587.955810546875,
      "learning_rate": 0.0008120000000000001,
      "loss": 0.7195,
      "step": 47000
    },
    {
      "epoch": 1.9,
      "grad_norm": 381.80517578125,
      "learning_rate": 0.0008100000000000001,
      "loss": 0.7537,
      "step": 47500
    },
    {
      "epoch": 1.92,
      "grad_norm": 144.209228515625,
      "learning_rate": 0.000808,
      "loss": 0.7228,
      "step": 48000
    },
    {
      "epoch": 1.94,
      "grad_norm": 86.45633697509766,
      "learning_rate": 0.0008060000000000001,
      "loss": 0.7481,
      "step": 48500
    },
    {
      "epoch": 1.96,
      "grad_norm": 106.03577423095703,
      "learning_rate": 0.000804,
      "loss": 0.7556,
      "step": 49000
    },
    {
      "epoch": 1.98,
      "grad_norm": 182.51702880859375,
      "learning_rate": 0.0008020000000000001,
      "loss": 0.7469,
      "step": 49500
    },
    {
      "epoch": 2.0,
      "grad_norm": 192.49668884277344,
      "learning_rate": 0.0008,
      "loss": 0.7631,
      "step": 50000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": {
        "accuracy": 0.85975
      },
      "eval_loss": 0.6808730363845825,
      "eval_runtime": 41.1541,
      "eval_samples_per_second": 485.979,
      "eval_steps_per_second": 121.495,
      "step": 50000
    },
    {
      "epoch": 2.02,
      "grad_norm": 1645.394287109375,
      "learning_rate": 0.0007980000000000001,
      "loss": 0.7506,
      "step": 50500
    },
    {
      "epoch": 2.04,
      "grad_norm": 8.931267738342285,
      "learning_rate": 0.000796,
      "loss": 0.7275,
      "step": 51000
    },
    {
      "epoch": 2.06,
      "grad_norm": 451.49090576171875,
      "learning_rate": 0.0007940000000000001,
      "loss": 0.7258,
      "step": 51500
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.08174752444028854,
      "learning_rate": 0.0007920000000000001,
      "loss": 0.7481,
      "step": 52000
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.3556652069091797,
      "learning_rate": 0.00079,
      "loss": 0.7553,
      "step": 52500
    },
    {
      "epoch": 2.12,
      "grad_norm": 14.897370338439941,
      "learning_rate": 0.0007880000000000001,
      "loss": 0.7493,
      "step": 53000
    },
    {
      "epoch": 2.14,
      "grad_norm": 24.16037368774414,
      "learning_rate": 0.000786,
      "loss": 0.7424,
      "step": 53500
    },
    {
      "epoch": 2.16,
      "grad_norm": 419.95281982421875,
      "learning_rate": 0.0007840000000000001,
      "loss": 0.7699,
      "step": 54000
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.9126428365707397,
      "learning_rate": 0.000782,
      "loss": 0.8107,
      "step": 54500
    },
    {
      "epoch": 2.2,
      "grad_norm": 256.14599609375,
      "learning_rate": 0.0007800000000000001,
      "loss": 0.8315,
      "step": 55000
    },
    {
      "epoch": 2.22,
      "grad_norm": 448.97357177734375,
      "learning_rate": 0.000778,
      "loss": 0.6788,
      "step": 55500
    },
    {
      "epoch": 2.24,
      "grad_norm": 504.53424072265625,
      "learning_rate": 0.000776,
      "loss": 0.7949,
      "step": 56000
    },
    {
      "epoch": 2.26,
      "grad_norm": 6.191288948059082,
      "learning_rate": 0.0007740000000000001,
      "loss": 0.7242,
      "step": 56500
    },
    {
      "epoch": 2.28,
      "grad_norm": 739.7684326171875,
      "learning_rate": 0.000772,
      "loss": 0.7363,
      "step": 57000
    },
    {
      "epoch": 2.3,
      "grad_norm": 207.5795135498047,
      "learning_rate": 0.0007700000000000001,
      "loss": 0.7325,
      "step": 57500
    },
    {
      "epoch": 2.32,
      "grad_norm": 269.013916015625,
      "learning_rate": 0.000768,
      "loss": 0.8053,
      "step": 58000
    },
    {
      "epoch": 2.34,
      "grad_norm": 347.04608154296875,
      "learning_rate": 0.0007660000000000001,
      "loss": 0.7366,
      "step": 58500
    },
    {
      "epoch": 2.36,
      "grad_norm": 305.58056640625,
      "learning_rate": 0.000764,
      "loss": 0.7879,
      "step": 59000
    },
    {
      "epoch": 2.38,
      "grad_norm": 8.59219741821289,
      "learning_rate": 0.000762,
      "loss": 0.7706,
      "step": 59500
    },
    {
      "epoch": 2.4,
      "grad_norm": 1093.40673828125,
      "learning_rate": 0.00076,
      "loss": 0.7055,
      "step": 60000
    },
    {
      "epoch": 2.42,
      "grad_norm": 55.57994842529297,
      "learning_rate": 0.000758,
      "loss": 0.7994,
      "step": 60500
    },
    {
      "epoch": 2.44,
      "grad_norm": 35.18922805786133,
      "learning_rate": 0.000756,
      "loss": 0.7563,
      "step": 61000
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.88185715675354,
      "learning_rate": 0.000754,
      "loss": 0.7345,
      "step": 61500
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.2963314056396484,
      "learning_rate": 0.0007520000000000001,
      "loss": 0.7071,
      "step": 62000
    },
    {
      "epoch": 2.5,
      "grad_norm": 140.44546508789062,
      "learning_rate": 0.00075,
      "loss": 0.7493,
      "step": 62500
    },
    {
      "epoch": 2.52,
      "grad_norm": 1078.35986328125,
      "learning_rate": 0.000748,
      "loss": 0.7612,
      "step": 63000
    },
    {
      "epoch": 2.54,
      "grad_norm": 120.49620056152344,
      "learning_rate": 0.000746,
      "loss": 0.7992,
      "step": 63500
    },
    {
      "epoch": 2.56,
      "grad_norm": 140.32275390625,
      "learning_rate": 0.000744,
      "loss": 0.7754,
      "step": 64000
    },
    {
      "epoch": 2.58,
      "grad_norm": 513.8121337890625,
      "learning_rate": 0.000742,
      "loss": 0.7154,
      "step": 64500
    },
    {
      "epoch": 2.6,
      "grad_norm": 445.6025390625,
      "learning_rate": 0.00074,
      "loss": 0.8199,
      "step": 65000
    },
    {
      "epoch": 2.62,
      "grad_norm": 557.657958984375,
      "learning_rate": 0.000738,
      "loss": 0.7269,
      "step": 65500
    },
    {
      "epoch": 2.64,
      "grad_norm": 530.6468505859375,
      "learning_rate": 0.000736,
      "loss": 0.7245,
      "step": 66000
    },
    {
      "epoch": 2.66,
      "grad_norm": 935.8909912109375,
      "learning_rate": 0.000734,
      "loss": 0.7417,
      "step": 66500
    },
    {
      "epoch": 2.68,
      "grad_norm": 353.7798767089844,
      "learning_rate": 0.000732,
      "loss": 0.7196,
      "step": 67000
    },
    {
      "epoch": 2.7,
      "grad_norm": 264.1173095703125,
      "learning_rate": 0.00073,
      "loss": 0.7522,
      "step": 67500
    },
    {
      "epoch": 2.72,
      "grad_norm": 465.19451904296875,
      "learning_rate": 0.000728,
      "loss": 0.8764,
      "step": 68000
    },
    {
      "epoch": 2.74,
      "grad_norm": 1252.6422119140625,
      "learning_rate": 0.000726,
      "loss": 0.8052,
      "step": 68500
    },
    {
      "epoch": 2.76,
      "grad_norm": 231.36427307128906,
      "learning_rate": 0.000724,
      "loss": 0.7091,
      "step": 69000
    },
    {
      "epoch": 2.78,
      "grad_norm": 1346.1572265625,
      "learning_rate": 0.000722,
      "loss": 0.7707,
      "step": 69500
    },
    {
      "epoch": 2.8,
      "grad_norm": 263.8692932128906,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.7446,
      "step": 70000
    },
    {
      "epoch": 2.82,
      "grad_norm": 496.4037170410156,
      "learning_rate": 0.000718,
      "loss": 0.8652,
      "step": 70500
    },
    {
      "epoch": 2.84,
      "grad_norm": 545.7474365234375,
      "learning_rate": 0.000716,
      "loss": 0.7936,
      "step": 71000
    },
    {
      "epoch": 2.86,
      "grad_norm": 408.9639892578125,
      "learning_rate": 0.000714,
      "loss": 0.8068,
      "step": 71500
    },
    {
      "epoch": 2.88,
      "grad_norm": 295.8630065917969,
      "learning_rate": 0.000712,
      "loss": 0.7689,
      "step": 72000
    },
    {
      "epoch": 2.9,
      "grad_norm": 12.97110652923584,
      "learning_rate": 0.00071,
      "loss": 0.753,
      "step": 72500
    },
    {
      "epoch": 2.92,
      "grad_norm": 68.56658935546875,
      "learning_rate": 0.000708,
      "loss": 0.791,
      "step": 73000
    },
    {
      "epoch": 2.94,
      "grad_norm": 247.6959228515625,
      "learning_rate": 0.0007059999999999999,
      "loss": 0.7494,
      "step": 73500
    },
    {
      "epoch": 2.96,
      "grad_norm": 446.04052734375,
      "learning_rate": 0.000704,
      "loss": 0.8186,
      "step": 74000
    },
    {
      "epoch": 2.98,
      "grad_norm": 689.493896484375,
      "learning_rate": 0.0007019999999999999,
      "loss": 0.7471,
      "step": 74500
    },
    {
      "epoch": 3.0,
      "grad_norm": 382.2821960449219,
      "learning_rate": 0.0007,
      "loss": 0.818,
      "step": 75000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": {
        "accuracy": 0.8479
      },
      "eval_loss": 0.7645283937454224,
      "eval_runtime": 38.3246,
      "eval_samples_per_second": 521.858,
      "eval_steps_per_second": 130.464,
      "step": 75000
    }
  ],
  "logging_steps": 500,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 5622439450418112.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
