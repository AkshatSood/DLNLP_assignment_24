{
  "best_metric": 0.6766778230667114,
  "best_model_checkpoint": "./B/checkpoints/DistilBertUncased\\checkpoint-25000",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 150000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.08822648227214813,
      "learning_rate": 0.000998,
      "loss": 0.5146,
      "step": 500
    },
    {
      "epoch": 0.04,
      "grad_norm": 7.898756504058838,
      "learning_rate": 0.000996,
      "loss": 0.4484,
      "step": 1000
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.3587803840637207,
      "learning_rate": 0.000994,
      "loss": 0.4223,
      "step": 1500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.03922874853014946,
      "learning_rate": 0.000992,
      "loss": 0.4733,
      "step": 2000
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2478397935628891,
      "learning_rate": 0.00099,
      "loss": 0.4557,
      "step": 2500
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.367358207702637,
      "learning_rate": 0.000988,
      "loss": 0.4084,
      "step": 3000
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.264943927526474,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.4897,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.89871072769165,
      "learning_rate": 0.000984,
      "loss": 0.5166,
      "step": 4000
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2600928544998169,
      "learning_rate": 0.000982,
      "loss": 0.5035,
      "step": 4500
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.1040096282958984,
      "learning_rate": 0.00098,
      "loss": 0.4369,
      "step": 5000
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.964759349822998,
      "learning_rate": 0.000978,
      "loss": 0.4415,
      "step": 5500
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.2146785259246826,
      "learning_rate": 0.000976,
      "loss": 0.5569,
      "step": 6000
    },
    {
      "epoch": 0.26,
      "grad_norm": 34.042877197265625,
      "learning_rate": 0.000974,
      "loss": 0.4747,
      "step": 6500
    },
    {
      "epoch": 0.28,
      "grad_norm": 18.178424835205078,
      "learning_rate": 0.000972,
      "loss": 0.4769,
      "step": 7000
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.258935451507568,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.506,
      "step": 7500
    },
    {
      "epoch": 0.32,
      "grad_norm": 43.297447204589844,
      "learning_rate": 0.000968,
      "loss": 0.5244,
      "step": 8000
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.813616931438446,
      "learning_rate": 0.000966,
      "loss": 0.4894,
      "step": 8500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.9499895572662354,
      "learning_rate": 0.000964,
      "loss": 0.5357,
      "step": 9000
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09952063858509064,
      "learning_rate": 0.000962,
      "loss": 0.5248,
      "step": 9500
    },
    {
      "epoch": 0.4,
      "grad_norm": 25.061817169189453,
      "learning_rate": 0.00096,
      "loss": 0.5317,
      "step": 10000
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16409306228160858,
      "learning_rate": 0.000958,
      "loss": 0.4983,
      "step": 10500
    },
    {
      "epoch": 0.44,
      "grad_norm": 16.943370819091797,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.5527,
      "step": 11000
    },
    {
      "epoch": 0.46,
      "grad_norm": 53.3620491027832,
      "learning_rate": 0.000954,
      "loss": 0.519,
      "step": 11500
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.018714904785156,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.5125,
      "step": 12000
    },
    {
      "epoch": 0.5,
      "grad_norm": 22.545839309692383,
      "learning_rate": 0.00095,
      "loss": 0.528,
      "step": 12500
    },
    {
      "epoch": 0.52,
      "grad_norm": 59.90282440185547,
      "learning_rate": 0.000948,
      "loss": 0.5427,
      "step": 13000
    },
    {
      "epoch": 0.54,
      "grad_norm": 19.3067569732666,
      "learning_rate": 0.000946,
      "loss": 0.6068,
      "step": 13500
    },
    {
      "epoch": 0.56,
      "grad_norm": 49.90065383911133,
      "learning_rate": 0.000944,
      "loss": 0.5879,
      "step": 14000
    },
    {
      "epoch": 0.58,
      "grad_norm": 14.237058639526367,
      "learning_rate": 0.000942,
      "loss": 0.5435,
      "step": 14500
    },
    {
      "epoch": 0.6,
      "grad_norm": 147.92279052734375,
      "learning_rate": 0.00094,
      "loss": 0.5888,
      "step": 15000
    },
    {
      "epoch": 0.62,
      "grad_norm": 69.154541015625,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.5858,
      "step": 15500
    },
    {
      "epoch": 0.64,
      "grad_norm": 67.72359466552734,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.5002,
      "step": 16000
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.2266484498977661,
      "learning_rate": 0.000934,
      "loss": 0.5467,
      "step": 16500
    },
    {
      "epoch": 0.68,
      "grad_norm": 61.940452575683594,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.6364,
      "step": 17000
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.561913013458252,
      "learning_rate": 0.00093,
      "loss": 0.5913,
      "step": 17500
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.815735340118408,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5808,
      "step": 18000
    },
    {
      "epoch": 0.74,
      "grad_norm": 15.218993186950684,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.6192,
      "step": 18500
    },
    {
      "epoch": 0.76,
      "grad_norm": 50.52571487426758,
      "learning_rate": 0.000924,
      "loss": 0.5829,
      "step": 19000
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.213761329650879,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.6117,
      "step": 19500
    },
    {
      "epoch": 0.8,
      "grad_norm": 213.10328674316406,
      "learning_rate": 0.00092,
      "loss": 0.6177,
      "step": 20000
    },
    {
      "epoch": 0.82,
      "grad_norm": 77.04730224609375,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.6149,
      "step": 20500
    },
    {
      "epoch": 0.84,
      "grad_norm": 80.98167419433594,
      "learning_rate": 0.000916,
      "loss": 0.6679,
      "step": 21000
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.009888905100524426,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.7067,
      "step": 21500
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.15099625289440155,
      "learning_rate": 0.000912,
      "loss": 0.6685,
      "step": 22000
    },
    {
      "epoch": 0.9,
      "grad_norm": 61.906639099121094,
      "learning_rate": 0.00091,
      "loss": 0.6185,
      "step": 22500
    },
    {
      "epoch": 0.92,
      "grad_norm": 13.067138671875,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.6316,
      "step": 23000
    },
    {
      "epoch": 0.94,
      "grad_norm": 96.27442169189453,
      "learning_rate": 0.000906,
      "loss": 0.669,
      "step": 23500
    },
    {
      "epoch": 0.96,
      "grad_norm": 196.68402099609375,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.646,
      "step": 24000
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.008594074286520481,
      "learning_rate": 0.000902,
      "loss": 0.6281,
      "step": 24500
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.33812716603279114,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.7169,
      "step": 25000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": {
        "accuracy": 0.86475
      },
      "eval_loss": 0.6766778230667114,
      "eval_runtime": 39.8645,
      "eval_samples_per_second": 501.7,
      "eval_steps_per_second": 125.425,
      "step": 25000
    },
    {
      "epoch": 1.02,
      "grad_norm": 6.549378395080566,
      "learning_rate": 0.000898,
      "loss": 0.6699,
      "step": 25500
    },
    {
      "epoch": 1.04,
      "grad_norm": 41.62549591064453,
      "learning_rate": 0.000896,
      "loss": 0.6529,
      "step": 26000
    },
    {
      "epoch": 1.06,
      "grad_norm": 11.249669075012207,
      "learning_rate": 0.000894,
      "loss": 0.6429,
      "step": 26500
    },
    {
      "epoch": 1.08,
      "grad_norm": 153.14169311523438,
      "learning_rate": 0.000892,
      "loss": 0.7126,
      "step": 27000
    },
    {
      "epoch": 1.1,
      "grad_norm": 314.3110656738281,
      "learning_rate": 0.0008900000000000001,
      "loss": 0.6844,
      "step": 27500
    },
    {
      "epoch": 1.12,
      "grad_norm": 57.603904724121094,
      "learning_rate": 0.000888,
      "loss": 0.6797,
      "step": 28000
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.8486515283584595,
      "learning_rate": 0.0008860000000000001,
      "loss": 0.6694,
      "step": 28500
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4092085361480713,
      "learning_rate": 0.000884,
      "loss": 0.6365,
      "step": 29000
    },
    {
      "epoch": 1.18,
      "grad_norm": 245.41900634765625,
      "learning_rate": 0.000882,
      "loss": 0.6567,
      "step": 29500
    },
    {
      "epoch": 1.2,
      "grad_norm": 292.0746765136719,
      "learning_rate": 0.00088,
      "loss": 0.7074,
      "step": 30000
    },
    {
      "epoch": 1.22,
      "grad_norm": 6.3823442459106445,
      "learning_rate": 0.000878,
      "loss": 0.6259,
      "step": 30500
    },
    {
      "epoch": 1.24,
      "grad_norm": 68.0105972290039,
      "learning_rate": 0.000876,
      "loss": 0.6563,
      "step": 31000
    },
    {
      "epoch": 1.26,
      "grad_norm": 158.60580444335938,
      "learning_rate": 0.000874,
      "loss": 0.6663,
      "step": 31500
    },
    {
      "epoch": 1.28,
      "grad_norm": 13.43741226196289,
      "learning_rate": 0.000872,
      "loss": 0.7001,
      "step": 32000
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.012239448726177216,
      "learning_rate": 0.00087,
      "loss": 0.6646,
      "step": 32500
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.30624276399612427,
      "learning_rate": 0.0008680000000000001,
      "loss": 0.6669,
      "step": 33000
    },
    {
      "epoch": 1.34,
      "grad_norm": 360.9072570800781,
      "learning_rate": 0.000866,
      "loss": 0.6768,
      "step": 33500
    },
    {
      "epoch": 1.36,
      "grad_norm": 543.7313842773438,
      "learning_rate": 0.000864,
      "loss": 0.7024,
      "step": 34000
    },
    {
      "epoch": 1.38,
      "grad_norm": 116.29761505126953,
      "learning_rate": 0.000862,
      "loss": 0.6754,
      "step": 34500
    },
    {
      "epoch": 1.4,
      "grad_norm": 381.50762939453125,
      "learning_rate": 0.00086,
      "loss": 0.6955,
      "step": 35000
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.2527366578578949,
      "learning_rate": 0.000858,
      "loss": 0.6654,
      "step": 35500
    },
    {
      "epoch": 1.44,
      "grad_norm": 90.5650863647461,
      "learning_rate": 0.000856,
      "loss": 0.7017,
      "step": 36000
    },
    {
      "epoch": 1.46,
      "grad_norm": 363.1059265136719,
      "learning_rate": 0.000854,
      "loss": 0.791,
      "step": 36500
    },
    {
      "epoch": 1.48,
      "grad_norm": 3.192150354385376,
      "learning_rate": 0.000852,
      "loss": 0.7117,
      "step": 37000
    },
    {
      "epoch": 1.5,
      "grad_norm": 12.035703659057617,
      "learning_rate": 0.00085,
      "loss": 0.7058,
      "step": 37500
    },
    {
      "epoch": 1.52,
      "grad_norm": 21.619050979614258,
      "learning_rate": 0.000848,
      "loss": 0.8358,
      "step": 38000
    },
    {
      "epoch": 1.54,
      "grad_norm": 70.71847534179688,
      "learning_rate": 0.000846,
      "loss": 0.7002,
      "step": 38500
    },
    {
      "epoch": 1.56,
      "grad_norm": 111.39851379394531,
      "learning_rate": 0.000844,
      "loss": 0.7167,
      "step": 39000
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.13962626457214355,
      "learning_rate": 0.000842,
      "loss": 0.7172,
      "step": 39500
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.9017668962478638,
      "learning_rate": 0.00084,
      "loss": 0.7689,
      "step": 40000
    },
    {
      "epoch": 1.62,
      "grad_norm": 465.99920654296875,
      "learning_rate": 0.000838,
      "loss": 0.7167,
      "step": 40500
    },
    {
      "epoch": 1.64,
      "grad_norm": 369.94158935546875,
      "learning_rate": 0.0008359999999999999,
      "loss": 0.77,
      "step": 41000
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.16351844370365143,
      "learning_rate": 0.000834,
      "loss": 0.6791,
      "step": 41500
    },
    {
      "epoch": 1.68,
      "grad_norm": 421.8947448730469,
      "learning_rate": 0.000832,
      "loss": 0.7557,
      "step": 42000
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.7784960269927979,
      "learning_rate": 0.00083,
      "loss": 0.7602,
      "step": 42500
    },
    {
      "epoch": 1.72,
      "grad_norm": 7.238518714904785,
      "learning_rate": 0.000828,
      "loss": 0.7109,
      "step": 43000
    },
    {
      "epoch": 1.74,
      "grad_norm": 179.2086639404297,
      "learning_rate": 0.000826,
      "loss": 0.7203,
      "step": 43500
    },
    {
      "epoch": 1.76,
      "grad_norm": 236.70103454589844,
      "learning_rate": 0.000824,
      "loss": 0.7706,
      "step": 44000
    },
    {
      "epoch": 1.78,
      "grad_norm": 90.3520736694336,
      "learning_rate": 0.0008219999999999999,
      "loss": 0.7539,
      "step": 44500
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.042320728302002,
      "learning_rate": 0.00082,
      "loss": 0.7098,
      "step": 45000
    },
    {
      "epoch": 1.82,
      "grad_norm": 4.1188249588012695,
      "learning_rate": 0.0008179999999999999,
      "loss": 0.6973,
      "step": 45500
    },
    {
      "epoch": 1.84,
      "grad_norm": 1.163643717765808,
      "learning_rate": 0.000816,
      "loss": 0.72,
      "step": 46000
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.07819953560829163,
      "learning_rate": 0.0008139999999999999,
      "loss": 0.7134,
      "step": 46500
    },
    {
      "epoch": 1.88,
      "grad_norm": 587.955810546875,
      "learning_rate": 0.0008120000000000001,
      "loss": 0.7195,
      "step": 47000
    },
    {
      "epoch": 1.9,
      "grad_norm": 381.80517578125,
      "learning_rate": 0.0008100000000000001,
      "loss": 0.7537,
      "step": 47500
    },
    {
      "epoch": 1.92,
      "grad_norm": 144.209228515625,
      "learning_rate": 0.000808,
      "loss": 0.7228,
      "step": 48000
    },
    {
      "epoch": 1.94,
      "grad_norm": 86.45633697509766,
      "learning_rate": 0.0008060000000000001,
      "loss": 0.7481,
      "step": 48500
    },
    {
      "epoch": 1.96,
      "grad_norm": 106.03577423095703,
      "learning_rate": 0.000804,
      "loss": 0.7556,
      "step": 49000
    },
    {
      "epoch": 1.98,
      "grad_norm": 182.51702880859375,
      "learning_rate": 0.0008020000000000001,
      "loss": 0.7469,
      "step": 49500
    },
    {
      "epoch": 2.0,
      "grad_norm": 192.49668884277344,
      "learning_rate": 0.0008,
      "loss": 0.7631,
      "step": 50000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": {
        "accuracy": 0.85975
      },
      "eval_loss": 0.6808730363845825,
      "eval_runtime": 41.1541,
      "eval_samples_per_second": 485.979,
      "eval_steps_per_second": 121.495,
      "step": 50000
    },
    {
      "epoch": 2.02,
      "grad_norm": 1645.394287109375,
      "learning_rate": 0.0007980000000000001,
      "loss": 0.7506,
      "step": 50500
    },
    {
      "epoch": 2.04,
      "grad_norm": 8.931267738342285,
      "learning_rate": 0.000796,
      "loss": 0.7275,
      "step": 51000
    },
    {
      "epoch": 2.06,
      "grad_norm": 451.49090576171875,
      "learning_rate": 0.0007940000000000001,
      "loss": 0.7258,
      "step": 51500
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.08174752444028854,
      "learning_rate": 0.0007920000000000001,
      "loss": 0.7481,
      "step": 52000
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.3556652069091797,
      "learning_rate": 0.00079,
      "loss": 0.7553,
      "step": 52500
    },
    {
      "epoch": 2.12,
      "grad_norm": 14.897370338439941,
      "learning_rate": 0.0007880000000000001,
      "loss": 0.7493,
      "step": 53000
    },
    {
      "epoch": 2.14,
      "grad_norm": 24.16037368774414,
      "learning_rate": 0.000786,
      "loss": 0.7424,
      "step": 53500
    },
    {
      "epoch": 2.16,
      "grad_norm": 419.95281982421875,
      "learning_rate": 0.0007840000000000001,
      "loss": 0.7699,
      "step": 54000
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.9126428365707397,
      "learning_rate": 0.000782,
      "loss": 0.8107,
      "step": 54500
    },
    {
      "epoch": 2.2,
      "grad_norm": 256.14599609375,
      "learning_rate": 0.0007800000000000001,
      "loss": 0.8315,
      "step": 55000
    },
    {
      "epoch": 2.22,
      "grad_norm": 448.97357177734375,
      "learning_rate": 0.000778,
      "loss": 0.6788,
      "step": 55500
    },
    {
      "epoch": 2.24,
      "grad_norm": 504.53424072265625,
      "learning_rate": 0.000776,
      "loss": 0.7949,
      "step": 56000
    },
    {
      "epoch": 2.26,
      "grad_norm": 6.191288948059082,
      "learning_rate": 0.0007740000000000001,
      "loss": 0.7242,
      "step": 56500
    },
    {
      "epoch": 2.28,
      "grad_norm": 739.7684326171875,
      "learning_rate": 0.000772,
      "loss": 0.7363,
      "step": 57000
    },
    {
      "epoch": 2.3,
      "grad_norm": 207.5795135498047,
      "learning_rate": 0.0007700000000000001,
      "loss": 0.7325,
      "step": 57500
    },
    {
      "epoch": 2.32,
      "grad_norm": 269.013916015625,
      "learning_rate": 0.000768,
      "loss": 0.8053,
      "step": 58000
    },
    {
      "epoch": 2.34,
      "grad_norm": 347.04608154296875,
      "learning_rate": 0.0007660000000000001,
      "loss": 0.7366,
      "step": 58500
    },
    {
      "epoch": 2.36,
      "grad_norm": 305.58056640625,
      "learning_rate": 0.000764,
      "loss": 0.7879,
      "step": 59000
    },
    {
      "epoch": 2.38,
      "grad_norm": 8.59219741821289,
      "learning_rate": 0.000762,
      "loss": 0.7706,
      "step": 59500
    },
    {
      "epoch": 2.4,
      "grad_norm": 1093.40673828125,
      "learning_rate": 0.00076,
      "loss": 0.7055,
      "step": 60000
    },
    {
      "epoch": 2.42,
      "grad_norm": 55.57994842529297,
      "learning_rate": 0.000758,
      "loss": 0.7994,
      "step": 60500
    },
    {
      "epoch": 2.44,
      "grad_norm": 35.18922805786133,
      "learning_rate": 0.000756,
      "loss": 0.7563,
      "step": 61000
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.88185715675354,
      "learning_rate": 0.000754,
      "loss": 0.7345,
      "step": 61500
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.2963314056396484,
      "learning_rate": 0.0007520000000000001,
      "loss": 0.7071,
      "step": 62000
    },
    {
      "epoch": 2.5,
      "grad_norm": 140.44546508789062,
      "learning_rate": 0.00075,
      "loss": 0.7493,
      "step": 62500
    },
    {
      "epoch": 2.52,
      "grad_norm": 1078.35986328125,
      "learning_rate": 0.000748,
      "loss": 0.7612,
      "step": 63000
    },
    {
      "epoch": 2.54,
      "grad_norm": 120.49620056152344,
      "learning_rate": 0.000746,
      "loss": 0.7992,
      "step": 63500
    },
    {
      "epoch": 2.56,
      "grad_norm": 140.32275390625,
      "learning_rate": 0.000744,
      "loss": 0.7754,
      "step": 64000
    },
    {
      "epoch": 2.58,
      "grad_norm": 513.8121337890625,
      "learning_rate": 0.000742,
      "loss": 0.7154,
      "step": 64500
    },
    {
      "epoch": 2.6,
      "grad_norm": 445.6025390625,
      "learning_rate": 0.00074,
      "loss": 0.8199,
      "step": 65000
    },
    {
      "epoch": 2.62,
      "grad_norm": 557.657958984375,
      "learning_rate": 0.000738,
      "loss": 0.7269,
      "step": 65500
    },
    {
      "epoch": 2.64,
      "grad_norm": 530.6468505859375,
      "learning_rate": 0.000736,
      "loss": 0.7245,
      "step": 66000
    },
    {
      "epoch": 2.66,
      "grad_norm": 935.8909912109375,
      "learning_rate": 0.000734,
      "loss": 0.7417,
      "step": 66500
    },
    {
      "epoch": 2.68,
      "grad_norm": 353.7798767089844,
      "learning_rate": 0.000732,
      "loss": 0.7196,
      "step": 67000
    },
    {
      "epoch": 2.7,
      "grad_norm": 264.1173095703125,
      "learning_rate": 0.00073,
      "loss": 0.7522,
      "step": 67500
    },
    {
      "epoch": 2.72,
      "grad_norm": 465.19451904296875,
      "learning_rate": 0.000728,
      "loss": 0.8764,
      "step": 68000
    },
    {
      "epoch": 2.74,
      "grad_norm": 1252.6422119140625,
      "learning_rate": 0.000726,
      "loss": 0.8052,
      "step": 68500
    },
    {
      "epoch": 2.76,
      "grad_norm": 231.36427307128906,
      "learning_rate": 0.000724,
      "loss": 0.7091,
      "step": 69000
    },
    {
      "epoch": 2.78,
      "grad_norm": 1346.1572265625,
      "learning_rate": 0.000722,
      "loss": 0.7707,
      "step": 69500
    },
    {
      "epoch": 2.8,
      "grad_norm": 263.8692932128906,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.7446,
      "step": 70000
    },
    {
      "epoch": 2.82,
      "grad_norm": 496.4037170410156,
      "learning_rate": 0.000718,
      "loss": 0.8652,
      "step": 70500
    },
    {
      "epoch": 2.84,
      "grad_norm": 545.7474365234375,
      "learning_rate": 0.000716,
      "loss": 0.7936,
      "step": 71000
    },
    {
      "epoch": 2.86,
      "grad_norm": 408.9639892578125,
      "learning_rate": 0.000714,
      "loss": 0.8068,
      "step": 71500
    },
    {
      "epoch": 2.88,
      "grad_norm": 295.8630065917969,
      "learning_rate": 0.000712,
      "loss": 0.7689,
      "step": 72000
    },
    {
      "epoch": 2.9,
      "grad_norm": 12.97110652923584,
      "learning_rate": 0.00071,
      "loss": 0.753,
      "step": 72500
    },
    {
      "epoch": 2.92,
      "grad_norm": 68.56658935546875,
      "learning_rate": 0.000708,
      "loss": 0.791,
      "step": 73000
    },
    {
      "epoch": 2.94,
      "grad_norm": 247.6959228515625,
      "learning_rate": 0.0007059999999999999,
      "loss": 0.7494,
      "step": 73500
    },
    {
      "epoch": 2.96,
      "grad_norm": 446.04052734375,
      "learning_rate": 0.000704,
      "loss": 0.8186,
      "step": 74000
    },
    {
      "epoch": 2.98,
      "grad_norm": 689.493896484375,
      "learning_rate": 0.0007019999999999999,
      "loss": 0.7471,
      "step": 74500
    },
    {
      "epoch": 3.0,
      "grad_norm": 382.2821960449219,
      "learning_rate": 0.0007,
      "loss": 0.818,
      "step": 75000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": {
        "accuracy": 0.8479
      },
      "eval_loss": 0.7645283937454224,
      "eval_runtime": 38.3246,
      "eval_samples_per_second": 521.858,
      "eval_steps_per_second": 130.464,
      "step": 75000
    },
    {
      "epoch": 3.02,
      "grad_norm": 34.856170654296875,
      "learning_rate": 0.0006979999999999999,
      "loss": 0.7842,
      "step": 75500
    },
    {
      "epoch": 3.04,
      "grad_norm": 4.359329700469971,
      "learning_rate": 0.000696,
      "loss": 0.8019,
      "step": 76000
    },
    {
      "epoch": 3.06,
      "grad_norm": 438.27093505859375,
      "learning_rate": 0.000694,
      "loss": 0.7326,
      "step": 76500
    },
    {
      "epoch": 3.08,
      "grad_norm": 381.7764892578125,
      "learning_rate": 0.000692,
      "loss": 0.7519,
      "step": 77000
    },
    {
      "epoch": 3.1,
      "grad_norm": 1168.9764404296875,
      "learning_rate": 0.00069,
      "loss": 0.7562,
      "step": 77500
    },
    {
      "epoch": 3.12,
      "grad_norm": 284.5892333984375,
      "learning_rate": 0.0006879999999999999,
      "loss": 0.7774,
      "step": 78000
    },
    {
      "epoch": 3.14,
      "grad_norm": 303.24951171875,
      "learning_rate": 0.0006860000000000001,
      "loss": 0.79,
      "step": 78500
    },
    {
      "epoch": 3.16,
      "grad_norm": 176.5148468017578,
      "learning_rate": 0.000684,
      "loss": 0.727,
      "step": 79000
    },
    {
      "epoch": 3.18,
      "grad_norm": 324.7411193847656,
      "learning_rate": 0.0006820000000000001,
      "loss": 0.7849,
      "step": 79500
    },
    {
      "epoch": 3.2,
      "grad_norm": 102.39532470703125,
      "learning_rate": 0.00068,
      "loss": 0.7776,
      "step": 80000
    },
    {
      "epoch": 3.22,
      "grad_norm": 8.317693710327148,
      "learning_rate": 0.0006780000000000001,
      "loss": 0.7314,
      "step": 80500
    },
    {
      "epoch": 3.24,
      "grad_norm": 388.10980224609375,
      "learning_rate": 0.0006760000000000001,
      "loss": 0.6971,
      "step": 81000
    },
    {
      "epoch": 3.26,
      "grad_norm": 4.6065287590026855,
      "learning_rate": 0.000674,
      "loss": 0.7161,
      "step": 81500
    },
    {
      "epoch": 3.28,
      "grad_norm": 812.8987426757812,
      "learning_rate": 0.0006720000000000001,
      "loss": 0.6795,
      "step": 82000
    },
    {
      "epoch": 3.3,
      "grad_norm": 246.3987579345703,
      "learning_rate": 0.00067,
      "loss": 0.7829,
      "step": 82500
    },
    {
      "epoch": 3.32,
      "grad_norm": 118.4791488647461,
      "learning_rate": 0.0006680000000000001,
      "loss": 0.8019,
      "step": 83000
    },
    {
      "epoch": 3.34,
      "grad_norm": 357.0204772949219,
      "learning_rate": 0.000666,
      "loss": 0.6855,
      "step": 83500
    },
    {
      "epoch": 3.36,
      "grad_norm": 238.29905700683594,
      "learning_rate": 0.0006640000000000001,
      "loss": 0.7232,
      "step": 84000
    },
    {
      "epoch": 3.38,
      "grad_norm": 398.4237976074219,
      "learning_rate": 0.000662,
      "loss": 0.8271,
      "step": 84500
    },
    {
      "epoch": 3.4,
      "grad_norm": 849.763671875,
      "learning_rate": 0.00066,
      "loss": 0.8027,
      "step": 85000
    },
    {
      "epoch": 3.42,
      "grad_norm": 441.2332458496094,
      "learning_rate": 0.0006580000000000001,
      "loss": 0.7035,
      "step": 85500
    },
    {
      "epoch": 3.44,
      "grad_norm": 221.03411865234375,
      "learning_rate": 0.000656,
      "loss": 0.7321,
      "step": 86000
    },
    {
      "epoch": 3.46,
      "grad_norm": 280.0106506347656,
      "learning_rate": 0.0006540000000000001,
      "loss": 0.7403,
      "step": 86500
    },
    {
      "epoch": 3.48,
      "grad_norm": 251.7000274658203,
      "learning_rate": 0.000652,
      "loss": 0.753,
      "step": 87000
    },
    {
      "epoch": 3.5,
      "grad_norm": 546.8394775390625,
      "learning_rate": 0.0006500000000000001,
      "loss": 0.7532,
      "step": 87500
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.9923107624053955,
      "learning_rate": 0.000648,
      "loss": 0.7401,
      "step": 88000
    },
    {
      "epoch": 3.54,
      "grad_norm": 611.3046264648438,
      "learning_rate": 0.000646,
      "loss": 0.7251,
      "step": 88500
    },
    {
      "epoch": 3.56,
      "grad_norm": 8.793217658996582,
      "learning_rate": 0.000644,
      "loss": 0.7574,
      "step": 89000
    },
    {
      "epoch": 3.58,
      "grad_norm": 111.47941589355469,
      "learning_rate": 0.000642,
      "loss": 0.7741,
      "step": 89500
    },
    {
      "epoch": 3.6,
      "grad_norm": 1034.6622314453125,
      "learning_rate": 0.00064,
      "loss": 0.684,
      "step": 90000
    },
    {
      "epoch": 3.62,
      "grad_norm": 25.55658531188965,
      "learning_rate": 0.000638,
      "loss": 0.7265,
      "step": 90500
    },
    {
      "epoch": 3.64,
      "grad_norm": 33.427947998046875,
      "learning_rate": 0.0006360000000000001,
      "loss": 0.7243,
      "step": 91000
    },
    {
      "epoch": 3.66,
      "grad_norm": 440.321044921875,
      "learning_rate": 0.000634,
      "loss": 0.7562,
      "step": 91500
    },
    {
      "epoch": 3.68,
      "grad_norm": 81.01660919189453,
      "learning_rate": 0.000632,
      "loss": 0.7349,
      "step": 92000
    },
    {
      "epoch": 3.7,
      "grad_norm": 3.426272392272949,
      "learning_rate": 0.00063,
      "loss": 0.6962,
      "step": 92500
    },
    {
      "epoch": 3.72,
      "grad_norm": 2184.64306640625,
      "learning_rate": 0.000628,
      "loss": 0.7768,
      "step": 93000
    },
    {
      "epoch": 3.74,
      "grad_norm": 11.006987571716309,
      "learning_rate": 0.000626,
      "loss": 0.7663,
      "step": 93500
    },
    {
      "epoch": 3.76,
      "grad_norm": 2498.764404296875,
      "learning_rate": 0.000624,
      "loss": 0.7789,
      "step": 94000
    },
    {
      "epoch": 3.78,
      "grad_norm": 7.189889907836914,
      "learning_rate": 0.000622,
      "loss": 0.767,
      "step": 94500
    },
    {
      "epoch": 3.8,
      "grad_norm": 29.47304916381836,
      "learning_rate": 0.00062,
      "loss": 0.7728,
      "step": 95000
    },
    {
      "epoch": 3.82,
      "grad_norm": 302.2178649902344,
      "learning_rate": 0.0006180000000000001,
      "loss": 0.7322,
      "step": 95500
    },
    {
      "epoch": 3.84,
      "grad_norm": 42.226722717285156,
      "learning_rate": 0.000616,
      "loss": 0.775,
      "step": 96000
    },
    {
      "epoch": 3.86,
      "grad_norm": 196.9398956298828,
      "learning_rate": 0.000614,
      "loss": 0.7562,
      "step": 96500
    },
    {
      "epoch": 3.88,
      "grad_norm": 8.422393798828125,
      "learning_rate": 0.000612,
      "loss": 0.746,
      "step": 97000
    },
    {
      "epoch": 3.9,
      "grad_norm": 109.93981170654297,
      "learning_rate": 0.00061,
      "loss": 0.8217,
      "step": 97500
    },
    {
      "epoch": 3.92,
      "grad_norm": 1948.95703125,
      "learning_rate": 0.000608,
      "loss": 0.7565,
      "step": 98000
    },
    {
      "epoch": 3.94,
      "grad_norm": 12.216506958007812,
      "learning_rate": 0.000606,
      "loss": 0.8079,
      "step": 98500
    },
    {
      "epoch": 3.96,
      "grad_norm": 774.7198486328125,
      "learning_rate": 0.000604,
      "loss": 0.7068,
      "step": 99000
    },
    {
      "epoch": 3.98,
      "grad_norm": 808.7705078125,
      "learning_rate": 0.000602,
      "loss": 0.7438,
      "step": 99500
    },
    {
      "epoch": 4.0,
      "grad_norm": 698.9755859375,
      "learning_rate": 0.0006,
      "loss": 0.7534,
      "step": 100000
    },
    {
      "epoch": 4.0,
      "eval_accuracy": {
        "accuracy": 0.851
      },
      "eval_loss": 0.7133814692497253,
      "eval_runtime": 39.1272,
      "eval_samples_per_second": 511.154,
      "eval_steps_per_second": 127.788,
      "step": 100000
    },
    {
      "epoch": 4.02,
      "grad_norm": 448.591064453125,
      "learning_rate": 0.000598,
      "loss": 0.7741,
      "step": 100500
    },
    {
      "epoch": 4.04,
      "grad_norm": 152.9704132080078,
      "learning_rate": 0.000596,
      "loss": 0.7555,
      "step": 101000
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.9609742760658264,
      "learning_rate": 0.000594,
      "loss": 0.756,
      "step": 101500
    },
    {
      "epoch": 4.08,
      "grad_norm": 303.1271667480469,
      "learning_rate": 0.000592,
      "loss": 0.7839,
      "step": 102000
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.5992464423179626,
      "learning_rate": 0.00059,
      "loss": 0.7981,
      "step": 102500
    },
    {
      "epoch": 4.12,
      "grad_norm": 2287.62255859375,
      "learning_rate": 0.000588,
      "loss": 0.7129,
      "step": 103000
    },
    {
      "epoch": 4.14,
      "grad_norm": 50.36689758300781,
      "learning_rate": 0.0005859999999999999,
      "loss": 0.7197,
      "step": 103500
    },
    {
      "epoch": 4.16,
      "grad_norm": 1721.1329345703125,
      "learning_rate": 0.000584,
      "loss": 0.744,
      "step": 104000
    },
    {
      "epoch": 4.18,
      "grad_norm": 2.1559488773345947,
      "learning_rate": 0.0005819999999999999,
      "loss": 0.7316,
      "step": 104500
    },
    {
      "epoch": 4.2,
      "grad_norm": 261.7562255859375,
      "learning_rate": 0.00058,
      "loss": 0.7741,
      "step": 105000
    },
    {
      "epoch": 4.22,
      "grad_norm": 11.875345230102539,
      "learning_rate": 0.000578,
      "loss": 0.7544,
      "step": 105500
    },
    {
      "epoch": 4.24,
      "grad_norm": 163.13311767578125,
      "learning_rate": 0.000576,
      "loss": 0.6927,
      "step": 106000
    },
    {
      "epoch": 4.26,
      "grad_norm": 225.0519561767578,
      "learning_rate": 0.000574,
      "loss": 0.7465,
      "step": 106500
    },
    {
      "epoch": 4.28,
      "grad_norm": 80.15457153320312,
      "learning_rate": 0.0005719999999999999,
      "loss": 0.7448,
      "step": 107000
    },
    {
      "epoch": 4.3,
      "grad_norm": 2.617668390274048,
      "learning_rate": 0.00057,
      "loss": 0.8114,
      "step": 107500
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.6164417862892151,
      "learning_rate": 0.0005679999999999999,
      "loss": 0.7284,
      "step": 108000
    },
    {
      "epoch": 4.34,
      "grad_norm": 161.5952911376953,
      "learning_rate": 0.000566,
      "loss": 0.7912,
      "step": 108500
    },
    {
      "epoch": 4.36,
      "grad_norm": 326.0581359863281,
      "learning_rate": 0.0005639999999999999,
      "loss": 0.7448,
      "step": 109000
    },
    {
      "epoch": 4.38,
      "grad_norm": 251.7965545654297,
      "learning_rate": 0.0005620000000000001,
      "loss": 0.7521,
      "step": 109500
    },
    {
      "epoch": 4.4,
      "grad_norm": 360.9570007324219,
      "learning_rate": 0.0005600000000000001,
      "loss": 0.7126,
      "step": 110000
    },
    {
      "epoch": 4.42,
      "grad_norm": 1.7385978698730469,
      "learning_rate": 0.000558,
      "loss": 0.7032,
      "step": 110500
    },
    {
      "epoch": 4.44,
      "grad_norm": 186.65272521972656,
      "learning_rate": 0.0005560000000000001,
      "loss": 0.8403,
      "step": 111000
    },
    {
      "epoch": 4.46,
      "grad_norm": 460.61016845703125,
      "learning_rate": 0.000554,
      "loss": 0.7348,
      "step": 111500
    },
    {
      "epoch": 4.48,
      "grad_norm": 360.7991943359375,
      "learning_rate": 0.0005520000000000001,
      "loss": 0.7679,
      "step": 112000
    },
    {
      "epoch": 4.5,
      "grad_norm": 115.23887634277344,
      "learning_rate": 0.00055,
      "loss": 0.7393,
      "step": 112500
    },
    {
      "epoch": 4.52,
      "grad_norm": 182.8375244140625,
      "learning_rate": 0.0005480000000000001,
      "loss": 0.7418,
      "step": 113000
    },
    {
      "epoch": 4.54,
      "grad_norm": 37.827030181884766,
      "learning_rate": 0.000546,
      "loss": 0.8027,
      "step": 113500
    },
    {
      "epoch": 4.56,
      "grad_norm": 4.889217376708984,
      "learning_rate": 0.0005440000000000001,
      "loss": 0.7939,
      "step": 114000
    },
    {
      "epoch": 4.58,
      "grad_norm": 283.3753967285156,
      "learning_rate": 0.0005420000000000001,
      "loss": 0.7919,
      "step": 114500
    },
    {
      "epoch": 4.6,
      "grad_norm": 211.84072875976562,
      "learning_rate": 0.00054,
      "loss": 0.6549,
      "step": 115000
    },
    {
      "epoch": 4.62,
      "grad_norm": 687.2828369140625,
      "learning_rate": 0.0005380000000000001,
      "loss": 0.7242,
      "step": 115500
    },
    {
      "epoch": 4.64,
      "grad_norm": 923.770263671875,
      "learning_rate": 0.000536,
      "loss": 0.7052,
      "step": 116000
    },
    {
      "epoch": 4.66,
      "grad_norm": 153.45733642578125,
      "learning_rate": 0.0005340000000000001,
      "loss": 0.6557,
      "step": 116500
    },
    {
      "epoch": 4.68,
      "grad_norm": 5.791944980621338,
      "learning_rate": 0.000532,
      "loss": 0.6932,
      "step": 117000
    },
    {
      "epoch": 4.7,
      "grad_norm": 9.518250465393066,
      "learning_rate": 0.0005300000000000001,
      "loss": 0.7737,
      "step": 117500
    },
    {
      "epoch": 4.72,
      "grad_norm": 553.18798828125,
      "learning_rate": 0.000528,
      "loss": 0.7316,
      "step": 118000
    },
    {
      "epoch": 4.74,
      "grad_norm": 41.08219909667969,
      "learning_rate": 0.000526,
      "loss": 0.7244,
      "step": 118500
    },
    {
      "epoch": 4.76,
      "grad_norm": 114.85610961914062,
      "learning_rate": 0.000524,
      "loss": 0.7377,
      "step": 119000
    },
    {
      "epoch": 4.78,
      "grad_norm": 387.1578369140625,
      "learning_rate": 0.000522,
      "loss": 0.7125,
      "step": 119500
    },
    {
      "epoch": 4.8,
      "grad_norm": 150.4223175048828,
      "learning_rate": 0.0005200000000000001,
      "loss": 0.7348,
      "step": 120000
    },
    {
      "epoch": 4.82,
      "grad_norm": 6.009800434112549,
      "learning_rate": 0.000518,
      "loss": 0.615,
      "step": 120500
    },
    {
      "epoch": 4.84,
      "grad_norm": 391.11175537109375,
      "learning_rate": 0.0005160000000000001,
      "loss": 0.7527,
      "step": 121000
    },
    {
      "epoch": 4.86,
      "grad_norm": 27.40836524963379,
      "learning_rate": 0.000514,
      "loss": 0.7302,
      "step": 121500
    },
    {
      "epoch": 4.88,
      "grad_norm": 173.09837341308594,
      "learning_rate": 0.000512,
      "loss": 0.6845,
      "step": 122000
    },
    {
      "epoch": 4.9,
      "grad_norm": 4512.09765625,
      "learning_rate": 0.00051,
      "loss": 0.7153,
      "step": 122500
    },
    {
      "epoch": 4.92,
      "grad_norm": 12.741973876953125,
      "learning_rate": 0.000508,
      "loss": 0.7422,
      "step": 123000
    },
    {
      "epoch": 4.94,
      "grad_norm": 774.936279296875,
      "learning_rate": 0.000506,
      "loss": 0.6882,
      "step": 123500
    },
    {
      "epoch": 4.96,
      "grad_norm": 222.51087951660156,
      "learning_rate": 0.000504,
      "loss": 0.7313,
      "step": 124000
    },
    {
      "epoch": 4.98,
      "grad_norm": 189.41856384277344,
      "learning_rate": 0.0005020000000000001,
      "loss": 0.7356,
      "step": 124500
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.06422700732946396,
      "learning_rate": 0.0005,
      "loss": 0.7069,
      "step": 125000
    },
    {
      "epoch": 5.0,
      "eval_accuracy": {
        "accuracy": 0.85385
      },
      "eval_loss": 0.7499802708625793,
      "eval_runtime": 41.3601,
      "eval_samples_per_second": 483.557,
      "eval_steps_per_second": 120.889,
      "step": 125000
    },
    {
      "epoch": 5.02,
      "grad_norm": 316.57177734375,
      "learning_rate": 0.000498,
      "loss": 0.7387,
      "step": 125500
    },
    {
      "epoch": 5.04,
      "grad_norm": 42.03173828125,
      "learning_rate": 0.000496,
      "loss": 0.6844,
      "step": 126000
    },
    {
      "epoch": 5.06,
      "grad_norm": 72.66574096679688,
      "learning_rate": 0.000494,
      "loss": 0.733,
      "step": 126500
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.3442893624305725,
      "learning_rate": 0.000492,
      "loss": 0.6983,
      "step": 127000
    },
    {
      "epoch": 5.1,
      "grad_norm": 582.3744506835938,
      "learning_rate": 0.00049,
      "loss": 0.7644,
      "step": 127500
    },
    {
      "epoch": 5.12,
      "grad_norm": 302.3772888183594,
      "learning_rate": 0.000488,
      "loss": 0.7004,
      "step": 128000
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.10566936433315277,
      "learning_rate": 0.000486,
      "loss": 0.7017,
      "step": 128500
    },
    {
      "epoch": 5.16,
      "grad_norm": 2780.4296875,
      "learning_rate": 0.000484,
      "loss": 0.7474,
      "step": 129000
    },
    {
      "epoch": 5.18,
      "grad_norm": 13.419196128845215,
      "learning_rate": 0.000482,
      "loss": 0.7658,
      "step": 129500
    },
    {
      "epoch": 5.2,
      "grad_norm": 545.4178466796875,
      "learning_rate": 0.00048,
      "loss": 0.6418,
      "step": 130000
    },
    {
      "epoch": 5.22,
      "grad_norm": 73.84529113769531,
      "learning_rate": 0.00047799999999999996,
      "loss": 0.7659,
      "step": 130500
    },
    {
      "epoch": 5.24,
      "grad_norm": 145.0167694091797,
      "learning_rate": 0.00047599999999999997,
      "loss": 0.6615,
      "step": 131000
    },
    {
      "epoch": 5.26,
      "grad_norm": 75.46564483642578,
      "learning_rate": 0.000474,
      "loss": 0.7098,
      "step": 131500
    },
    {
      "epoch": 5.28,
      "grad_norm": 389.6264953613281,
      "learning_rate": 0.000472,
      "loss": 0.7268,
      "step": 132000
    },
    {
      "epoch": 5.3,
      "grad_norm": 167.6171875,
      "learning_rate": 0.00047,
      "loss": 0.7308,
      "step": 132500
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.1777647733688354,
      "learning_rate": 0.00046800000000000005,
      "loss": 0.6993,
      "step": 133000
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.627841591835022,
      "learning_rate": 0.00046600000000000005,
      "loss": 0.7053,
      "step": 133500
    },
    {
      "epoch": 5.36,
      "grad_norm": 10.954928398132324,
      "learning_rate": 0.00046400000000000006,
      "loss": 0.7228,
      "step": 134000
    },
    {
      "epoch": 5.38,
      "grad_norm": 6.693686008453369,
      "learning_rate": 0.000462,
      "loss": 0.6623,
      "step": 134500
    },
    {
      "epoch": 5.4,
      "grad_norm": 213.9030303955078,
      "learning_rate": 0.00046,
      "loss": 0.708,
      "step": 135000
    },
    {
      "epoch": 5.42,
      "grad_norm": 371.24920654296875,
      "learning_rate": 0.000458,
      "loss": 0.7973,
      "step": 135500
    },
    {
      "epoch": 5.44,
      "grad_norm": 71.06494903564453,
      "learning_rate": 0.000456,
      "loss": 0.6969,
      "step": 136000
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.12395527958869934,
      "learning_rate": 0.00045400000000000003,
      "loss": 0.7677,
      "step": 136500
    },
    {
      "epoch": 5.48,
      "grad_norm": 224.33010864257812,
      "learning_rate": 0.00045200000000000004,
      "loss": 0.6891,
      "step": 137000
    },
    {
      "epoch": 5.5,
      "grad_norm": 115.3927001953125,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.7207,
      "step": 137500
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.27870404720306396,
      "learning_rate": 0.000448,
      "loss": 0.67,
      "step": 138000
    },
    {
      "epoch": 5.54,
      "grad_norm": 755.419189453125,
      "learning_rate": 0.000446,
      "loss": 0.7581,
      "step": 138500
    },
    {
      "epoch": 5.56,
      "grad_norm": 902.3057861328125,
      "learning_rate": 0.000444,
      "loss": 0.7252,
      "step": 139000
    },
    {
      "epoch": 5.58,
      "grad_norm": 23.86826515197754,
      "learning_rate": 0.000442,
      "loss": 0.7056,
      "step": 139500
    },
    {
      "epoch": 5.6,
      "grad_norm": 241.62547302246094,
      "learning_rate": 0.00044,
      "loss": 0.8001,
      "step": 140000
    },
    {
      "epoch": 5.62,
      "grad_norm": 137.78839111328125,
      "learning_rate": 0.000438,
      "loss": 0.7488,
      "step": 140500
    },
    {
      "epoch": 5.64,
      "grad_norm": 259.2609558105469,
      "learning_rate": 0.000436,
      "loss": 0.7228,
      "step": 141000
    },
    {
      "epoch": 5.66,
      "grad_norm": 515.2636108398438,
      "learning_rate": 0.00043400000000000003,
      "loss": 0.7239,
      "step": 141500
    },
    {
      "epoch": 5.68,
      "grad_norm": 96.66551208496094,
      "learning_rate": 0.000432,
      "loss": 0.6939,
      "step": 142000
    },
    {
      "epoch": 5.7,
      "grad_norm": 491.6911315917969,
      "learning_rate": 0.00043,
      "loss": 0.724,
      "step": 142500
    },
    {
      "epoch": 5.72,
      "grad_norm": 3.3586432933807373,
      "learning_rate": 0.000428,
      "loss": 0.7529,
      "step": 143000
    },
    {
      "epoch": 5.74,
      "grad_norm": 161.73057556152344,
      "learning_rate": 0.000426,
      "loss": 0.7004,
      "step": 143500
    },
    {
      "epoch": 5.76,
      "grad_norm": 2.058309555053711,
      "learning_rate": 0.000424,
      "loss": 0.7326,
      "step": 144000
    },
    {
      "epoch": 5.78,
      "grad_norm": 217.74794006347656,
      "learning_rate": 0.000422,
      "loss": 0.6995,
      "step": 144500
    },
    {
      "epoch": 5.8,
      "grad_norm": 1673.010498046875,
      "learning_rate": 0.00042,
      "loss": 0.6807,
      "step": 145000
    },
    {
      "epoch": 5.82,
      "grad_norm": 62.84111785888672,
      "learning_rate": 0.00041799999999999997,
      "loss": 0.7123,
      "step": 145500
    },
    {
      "epoch": 5.84,
      "grad_norm": 230.7152557373047,
      "learning_rate": 0.000416,
      "loss": 0.712,
      "step": 146000
    },
    {
      "epoch": 5.86,
      "grad_norm": 246.16249084472656,
      "learning_rate": 0.000414,
      "loss": 0.6983,
      "step": 146500
    },
    {
      "epoch": 5.88,
      "grad_norm": 7.739560127258301,
      "learning_rate": 0.000412,
      "loss": 0.6816,
      "step": 147000
    },
    {
      "epoch": 5.9,
      "grad_norm": 465.3836364746094,
      "learning_rate": 0.00041,
      "loss": 0.731,
      "step": 147500
    },
    {
      "epoch": 5.92,
      "grad_norm": 159.96543884277344,
      "learning_rate": 0.000408,
      "loss": 0.8047,
      "step": 148000
    },
    {
      "epoch": 5.94,
      "grad_norm": 312.50732421875,
      "learning_rate": 0.00040600000000000006,
      "loss": 0.6853,
      "step": 148500
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.4150084853172302,
      "learning_rate": 0.000404,
      "loss": 0.7287,
      "step": 149000
    },
    {
      "epoch": 5.98,
      "grad_norm": 227.0913848876953,
      "learning_rate": 0.000402,
      "loss": 0.7029,
      "step": 149500
    },
    {
      "epoch": 6.0,
      "grad_norm": 361.9114074707031,
      "learning_rate": 0.0004,
      "loss": 0.8364,
      "step": 150000
    },
    {
      "epoch": 6.0,
      "eval_accuracy": {
        "accuracy": 0.8563
      },
      "eval_loss": 0.7079243659973145,
      "eval_runtime": 45.1663,
      "eval_samples_per_second": 442.808,
      "eval_steps_per_second": 110.702,
      "step": 150000
    }
  ],
  "logging_steps": 500,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 1.1245729458534144e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
