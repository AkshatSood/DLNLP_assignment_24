{
  "best_metric": 0.6766778230667114,
  "best_model_checkpoint": "./B/checkpoints/DistilBertUncased\\checkpoint-25000",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 25000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.08822648227214813,
      "learning_rate": 0.000998,
      "loss": 0.5146,
      "step": 500
    },
    {
      "epoch": 0.04,
      "grad_norm": 7.898756504058838,
      "learning_rate": 0.000996,
      "loss": 0.4484,
      "step": 1000
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.3587803840637207,
      "learning_rate": 0.000994,
      "loss": 0.4223,
      "step": 1500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.03922874853014946,
      "learning_rate": 0.000992,
      "loss": 0.4733,
      "step": 2000
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2478397935628891,
      "learning_rate": 0.00099,
      "loss": 0.4557,
      "step": 2500
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.367358207702637,
      "learning_rate": 0.000988,
      "loss": 0.4084,
      "step": 3000
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.264943927526474,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.4897,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.89871072769165,
      "learning_rate": 0.000984,
      "loss": 0.5166,
      "step": 4000
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2600928544998169,
      "learning_rate": 0.000982,
      "loss": 0.5035,
      "step": 4500
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.1040096282958984,
      "learning_rate": 0.00098,
      "loss": 0.4369,
      "step": 5000
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.964759349822998,
      "learning_rate": 0.000978,
      "loss": 0.4415,
      "step": 5500
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.2146785259246826,
      "learning_rate": 0.000976,
      "loss": 0.5569,
      "step": 6000
    },
    {
      "epoch": 0.26,
      "grad_norm": 34.042877197265625,
      "learning_rate": 0.000974,
      "loss": 0.4747,
      "step": 6500
    },
    {
      "epoch": 0.28,
      "grad_norm": 18.178424835205078,
      "learning_rate": 0.000972,
      "loss": 0.4769,
      "step": 7000
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.258935451507568,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.506,
      "step": 7500
    },
    {
      "epoch": 0.32,
      "grad_norm": 43.297447204589844,
      "learning_rate": 0.000968,
      "loss": 0.5244,
      "step": 8000
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.813616931438446,
      "learning_rate": 0.000966,
      "loss": 0.4894,
      "step": 8500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.9499895572662354,
      "learning_rate": 0.000964,
      "loss": 0.5357,
      "step": 9000
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09952063858509064,
      "learning_rate": 0.000962,
      "loss": 0.5248,
      "step": 9500
    },
    {
      "epoch": 0.4,
      "grad_norm": 25.061817169189453,
      "learning_rate": 0.00096,
      "loss": 0.5317,
      "step": 10000
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16409306228160858,
      "learning_rate": 0.000958,
      "loss": 0.4983,
      "step": 10500
    },
    {
      "epoch": 0.44,
      "grad_norm": 16.943370819091797,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.5527,
      "step": 11000
    },
    {
      "epoch": 0.46,
      "grad_norm": 53.3620491027832,
      "learning_rate": 0.000954,
      "loss": 0.519,
      "step": 11500
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.018714904785156,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.5125,
      "step": 12000
    },
    {
      "epoch": 0.5,
      "grad_norm": 22.545839309692383,
      "learning_rate": 0.00095,
      "loss": 0.528,
      "step": 12500
    },
    {
      "epoch": 0.52,
      "grad_norm": 59.90282440185547,
      "learning_rate": 0.000948,
      "loss": 0.5427,
      "step": 13000
    },
    {
      "epoch": 0.54,
      "grad_norm": 19.3067569732666,
      "learning_rate": 0.000946,
      "loss": 0.6068,
      "step": 13500
    },
    {
      "epoch": 0.56,
      "grad_norm": 49.90065383911133,
      "learning_rate": 0.000944,
      "loss": 0.5879,
      "step": 14000
    },
    {
      "epoch": 0.58,
      "grad_norm": 14.237058639526367,
      "learning_rate": 0.000942,
      "loss": 0.5435,
      "step": 14500
    },
    {
      "epoch": 0.6,
      "grad_norm": 147.92279052734375,
      "learning_rate": 0.00094,
      "loss": 0.5888,
      "step": 15000
    },
    {
      "epoch": 0.62,
      "grad_norm": 69.154541015625,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.5858,
      "step": 15500
    },
    {
      "epoch": 0.64,
      "grad_norm": 67.72359466552734,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.5002,
      "step": 16000
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.2266484498977661,
      "learning_rate": 0.000934,
      "loss": 0.5467,
      "step": 16500
    },
    {
      "epoch": 0.68,
      "grad_norm": 61.940452575683594,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.6364,
      "step": 17000
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.561913013458252,
      "learning_rate": 0.00093,
      "loss": 0.5913,
      "step": 17500
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.815735340118408,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5808,
      "step": 18000
    },
    {
      "epoch": 0.74,
      "grad_norm": 15.218993186950684,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.6192,
      "step": 18500
    },
    {
      "epoch": 0.76,
      "grad_norm": 50.52571487426758,
      "learning_rate": 0.000924,
      "loss": 0.5829,
      "step": 19000
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.213761329650879,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.6117,
      "step": 19500
    },
    {
      "epoch": 0.8,
      "grad_norm": 213.10328674316406,
      "learning_rate": 0.00092,
      "loss": 0.6177,
      "step": 20000
    },
    {
      "epoch": 0.82,
      "grad_norm": 77.04730224609375,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.6149,
      "step": 20500
    },
    {
      "epoch": 0.84,
      "grad_norm": 80.98167419433594,
      "learning_rate": 0.000916,
      "loss": 0.6679,
      "step": 21000
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.009888905100524426,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.7067,
      "step": 21500
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.15099625289440155,
      "learning_rate": 0.000912,
      "loss": 0.6685,
      "step": 22000
    },
    {
      "epoch": 0.9,
      "grad_norm": 61.906639099121094,
      "learning_rate": 0.00091,
      "loss": 0.6185,
      "step": 22500
    },
    {
      "epoch": 0.92,
      "grad_norm": 13.067138671875,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.6316,
      "step": 23000
    },
    {
      "epoch": 0.94,
      "grad_norm": 96.27442169189453,
      "learning_rate": 0.000906,
      "loss": 0.669,
      "step": 23500
    },
    {
      "epoch": 0.96,
      "grad_norm": 196.68402099609375,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.646,
      "step": 24000
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.008594074286520481,
      "learning_rate": 0.000902,
      "loss": 0.6281,
      "step": 24500
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.33812716603279114,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.7169,
      "step": 25000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": {
        "accuracy": 0.86475
      },
      "eval_loss": 0.6766778230667114,
      "eval_runtime": 39.8645,
      "eval_samples_per_second": 501.7,
      "eval_steps_per_second": 125.425,
      "step": 25000
    }
  ],
  "logging_steps": 500,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 1874708971629312.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
