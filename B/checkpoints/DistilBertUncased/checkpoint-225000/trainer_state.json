{
  "best_metric": 0.5612757802009583,
  "best_model_checkpoint": "./B/checkpoints/DistilBertUncased\\checkpoint-225000",
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 225000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 0.08822648227214813,
      "learning_rate": 0.000998,
      "loss": 0.5146,
      "step": 500
    },
    {
      "epoch": 0.04,
      "grad_norm": 7.898756504058838,
      "learning_rate": 0.000996,
      "loss": 0.4484,
      "step": 1000
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.3587803840637207,
      "learning_rate": 0.000994,
      "loss": 0.4223,
      "step": 1500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.03922874853014946,
      "learning_rate": 0.000992,
      "loss": 0.4733,
      "step": 2000
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.2478397935628891,
      "learning_rate": 0.00099,
      "loss": 0.4557,
      "step": 2500
    },
    {
      "epoch": 0.12,
      "grad_norm": 12.367358207702637,
      "learning_rate": 0.000988,
      "loss": 0.4084,
      "step": 3000
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.264943927526474,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.4897,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.89871072769165,
      "learning_rate": 0.000984,
      "loss": 0.5166,
      "step": 4000
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.2600928544998169,
      "learning_rate": 0.000982,
      "loss": 0.5035,
      "step": 4500
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.1040096282958984,
      "learning_rate": 0.00098,
      "loss": 0.4369,
      "step": 5000
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.964759349822998,
      "learning_rate": 0.000978,
      "loss": 0.4415,
      "step": 5500
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.2146785259246826,
      "learning_rate": 0.000976,
      "loss": 0.5569,
      "step": 6000
    },
    {
      "epoch": 0.26,
      "grad_norm": 34.042877197265625,
      "learning_rate": 0.000974,
      "loss": 0.4747,
      "step": 6500
    },
    {
      "epoch": 0.28,
      "grad_norm": 18.178424835205078,
      "learning_rate": 0.000972,
      "loss": 0.4769,
      "step": 7000
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.258935451507568,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.506,
      "step": 7500
    },
    {
      "epoch": 0.32,
      "grad_norm": 43.297447204589844,
      "learning_rate": 0.000968,
      "loss": 0.5244,
      "step": 8000
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.813616931438446,
      "learning_rate": 0.000966,
      "loss": 0.4894,
      "step": 8500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.9499895572662354,
      "learning_rate": 0.000964,
      "loss": 0.5357,
      "step": 9000
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.09952063858509064,
      "learning_rate": 0.000962,
      "loss": 0.5248,
      "step": 9500
    },
    {
      "epoch": 0.4,
      "grad_norm": 25.061817169189453,
      "learning_rate": 0.00096,
      "loss": 0.5317,
      "step": 10000
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.16409306228160858,
      "learning_rate": 0.000958,
      "loss": 0.4983,
      "step": 10500
    },
    {
      "epoch": 0.44,
      "grad_norm": 16.943370819091797,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.5527,
      "step": 11000
    },
    {
      "epoch": 0.46,
      "grad_norm": 53.3620491027832,
      "learning_rate": 0.000954,
      "loss": 0.519,
      "step": 11500
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.018714904785156,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.5125,
      "step": 12000
    },
    {
      "epoch": 0.5,
      "grad_norm": 22.545839309692383,
      "learning_rate": 0.00095,
      "loss": 0.528,
      "step": 12500
    },
    {
      "epoch": 0.52,
      "grad_norm": 59.90282440185547,
      "learning_rate": 0.000948,
      "loss": 0.5427,
      "step": 13000
    },
    {
      "epoch": 0.54,
      "grad_norm": 19.3067569732666,
      "learning_rate": 0.000946,
      "loss": 0.6068,
      "step": 13500
    },
    {
      "epoch": 0.56,
      "grad_norm": 49.90065383911133,
      "learning_rate": 0.000944,
      "loss": 0.5879,
      "step": 14000
    },
    {
      "epoch": 0.58,
      "grad_norm": 14.237058639526367,
      "learning_rate": 0.000942,
      "loss": 0.5435,
      "step": 14500
    },
    {
      "epoch": 0.6,
      "grad_norm": 147.92279052734375,
      "learning_rate": 0.00094,
      "loss": 0.5888,
      "step": 15000
    },
    {
      "epoch": 0.62,
      "grad_norm": 69.154541015625,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.5858,
      "step": 15500
    },
    {
      "epoch": 0.64,
      "grad_norm": 67.72359466552734,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.5002,
      "step": 16000
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.2266484498977661,
      "learning_rate": 0.000934,
      "loss": 0.5467,
      "step": 16500
    },
    {
      "epoch": 0.68,
      "grad_norm": 61.940452575683594,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.6364,
      "step": 17000
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.561913013458252,
      "learning_rate": 0.00093,
      "loss": 0.5913,
      "step": 17500
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.815735340118408,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5808,
      "step": 18000
    },
    {
      "epoch": 0.74,
      "grad_norm": 15.218993186950684,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.6192,
      "step": 18500
    },
    {
      "epoch": 0.76,
      "grad_norm": 50.52571487426758,
      "learning_rate": 0.000924,
      "loss": 0.5829,
      "step": 19000
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.213761329650879,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.6117,
      "step": 19500
    },
    {
      "epoch": 0.8,
      "grad_norm": 213.10328674316406,
      "learning_rate": 0.00092,
      "loss": 0.6177,
      "step": 20000
    },
    {
      "epoch": 0.82,
      "grad_norm": 77.04730224609375,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.6149,
      "step": 20500
    },
    {
      "epoch": 0.84,
      "grad_norm": 80.98167419433594,
      "learning_rate": 0.000916,
      "loss": 0.6679,
      "step": 21000
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.009888905100524426,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.7067,
      "step": 21500
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.15099625289440155,
      "learning_rate": 0.000912,
      "loss": 0.6685,
      "step": 22000
    },
    {
      "epoch": 0.9,
      "grad_norm": 61.906639099121094,
      "learning_rate": 0.00091,
      "loss": 0.6185,
      "step": 22500
    },
    {
      "epoch": 0.92,
      "grad_norm": 13.067138671875,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.6316,
      "step": 23000
    },
    {
      "epoch": 0.94,
      "grad_norm": 96.27442169189453,
      "learning_rate": 0.000906,
      "loss": 0.669,
      "step": 23500
    },
    {
      "epoch": 0.96,
      "grad_norm": 196.68402099609375,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.646,
      "step": 24000
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.008594074286520481,
      "learning_rate": 0.000902,
      "loss": 0.6281,
      "step": 24500
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.33812716603279114,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.7169,
      "step": 25000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": {
        "accuracy": 0.86475
      },
      "eval_loss": 0.6766778230667114,
      "eval_runtime": 39.8645,
      "eval_samples_per_second": 501.7,
      "eval_steps_per_second": 125.425,
      "step": 25000
    },
    {
      "epoch": 1.02,
      "grad_norm": 6.549378395080566,
      "learning_rate": 0.000898,
      "loss": 0.6699,
      "step": 25500
    },
    {
      "epoch": 1.04,
      "grad_norm": 41.62549591064453,
      "learning_rate": 0.000896,
      "loss": 0.6529,
      "step": 26000
    },
    {
      "epoch": 1.06,
      "grad_norm": 11.249669075012207,
      "learning_rate": 0.000894,
      "loss": 0.6429,
      "step": 26500
    },
    {
      "epoch": 1.08,
      "grad_norm": 153.14169311523438,
      "learning_rate": 0.000892,
      "loss": 0.7126,
      "step": 27000
    },
    {
      "epoch": 1.1,
      "grad_norm": 314.3110656738281,
      "learning_rate": 0.0008900000000000001,
      "loss": 0.6844,
      "step": 27500
    },
    {
      "epoch": 1.12,
      "grad_norm": 57.603904724121094,
      "learning_rate": 0.000888,
      "loss": 0.6797,
      "step": 28000
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.8486515283584595,
      "learning_rate": 0.0008860000000000001,
      "loss": 0.6694,
      "step": 28500
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.4092085361480713,
      "learning_rate": 0.000884,
      "loss": 0.6365,
      "step": 29000
    },
    {
      "epoch": 1.18,
      "grad_norm": 245.41900634765625,
      "learning_rate": 0.000882,
      "loss": 0.6567,
      "step": 29500
    },
    {
      "epoch": 1.2,
      "grad_norm": 292.0746765136719,
      "learning_rate": 0.00088,
      "loss": 0.7074,
      "step": 30000
    },
    {
      "epoch": 1.22,
      "grad_norm": 6.3823442459106445,
      "learning_rate": 0.000878,
      "loss": 0.6259,
      "step": 30500
    },
    {
      "epoch": 1.24,
      "grad_norm": 68.0105972290039,
      "learning_rate": 0.000876,
      "loss": 0.6563,
      "step": 31000
    },
    {
      "epoch": 1.26,
      "grad_norm": 158.60580444335938,
      "learning_rate": 0.000874,
      "loss": 0.6663,
      "step": 31500
    },
    {
      "epoch": 1.28,
      "grad_norm": 13.43741226196289,
      "learning_rate": 0.000872,
      "loss": 0.7001,
      "step": 32000
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.012239448726177216,
      "learning_rate": 0.00087,
      "loss": 0.6646,
      "step": 32500
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.30624276399612427,
      "learning_rate": 0.0008680000000000001,
      "loss": 0.6669,
      "step": 33000
    },
    {
      "epoch": 1.34,
      "grad_norm": 360.9072570800781,
      "learning_rate": 0.000866,
      "loss": 0.6768,
      "step": 33500
    },
    {
      "epoch": 1.36,
      "grad_norm": 543.7313842773438,
      "learning_rate": 0.000864,
      "loss": 0.7024,
      "step": 34000
    },
    {
      "epoch": 1.38,
      "grad_norm": 116.29761505126953,
      "learning_rate": 0.000862,
      "loss": 0.6754,
      "step": 34500
    },
    {
      "epoch": 1.4,
      "grad_norm": 381.50762939453125,
      "learning_rate": 0.00086,
      "loss": 0.6955,
      "step": 35000
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.2527366578578949,
      "learning_rate": 0.000858,
      "loss": 0.6654,
      "step": 35500
    },
    {
      "epoch": 1.44,
      "grad_norm": 90.5650863647461,
      "learning_rate": 0.000856,
      "loss": 0.7017,
      "step": 36000
    },
    {
      "epoch": 1.46,
      "grad_norm": 363.1059265136719,
      "learning_rate": 0.000854,
      "loss": 0.791,
      "step": 36500
    },
    {
      "epoch": 1.48,
      "grad_norm": 3.192150354385376,
      "learning_rate": 0.000852,
      "loss": 0.7117,
      "step": 37000
    },
    {
      "epoch": 1.5,
      "grad_norm": 12.035703659057617,
      "learning_rate": 0.00085,
      "loss": 0.7058,
      "step": 37500
    },
    {
      "epoch": 1.52,
      "grad_norm": 21.619050979614258,
      "learning_rate": 0.000848,
      "loss": 0.8358,
      "step": 38000
    },
    {
      "epoch": 1.54,
      "grad_norm": 70.71847534179688,
      "learning_rate": 0.000846,
      "loss": 0.7002,
      "step": 38500
    },
    {
      "epoch": 1.56,
      "grad_norm": 111.39851379394531,
      "learning_rate": 0.000844,
      "loss": 0.7167,
      "step": 39000
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.13962626457214355,
      "learning_rate": 0.000842,
      "loss": 0.7172,
      "step": 39500
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.9017668962478638,
      "learning_rate": 0.00084,
      "loss": 0.7689,
      "step": 40000
    },
    {
      "epoch": 1.62,
      "grad_norm": 465.99920654296875,
      "learning_rate": 0.000838,
      "loss": 0.7167,
      "step": 40500
    },
    {
      "epoch": 1.64,
      "grad_norm": 369.94158935546875,
      "learning_rate": 0.0008359999999999999,
      "loss": 0.77,
      "step": 41000
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.16351844370365143,
      "learning_rate": 0.000834,
      "loss": 0.6791,
      "step": 41500
    },
    {
      "epoch": 1.68,
      "grad_norm": 421.8947448730469,
      "learning_rate": 0.000832,
      "loss": 0.7557,
      "step": 42000
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.7784960269927979,
      "learning_rate": 0.00083,
      "loss": 0.7602,
      "step": 42500
    },
    {
      "epoch": 1.72,
      "grad_norm": 7.238518714904785,
      "learning_rate": 0.000828,
      "loss": 0.7109,
      "step": 43000
    },
    {
      "epoch": 1.74,
      "grad_norm": 179.2086639404297,
      "learning_rate": 0.000826,
      "loss": 0.7203,
      "step": 43500
    },
    {
      "epoch": 1.76,
      "grad_norm": 236.70103454589844,
      "learning_rate": 0.000824,
      "loss": 0.7706,
      "step": 44000
    },
    {
      "epoch": 1.78,
      "grad_norm": 90.3520736694336,
      "learning_rate": 0.0008219999999999999,
      "loss": 0.7539,
      "step": 44500
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.042320728302002,
      "learning_rate": 0.00082,
      "loss": 0.7098,
      "step": 45000
    },
    {
      "epoch": 1.82,
      "grad_norm": 4.1188249588012695,
      "learning_rate": 0.0008179999999999999,
      "loss": 0.6973,
      "step": 45500
    },
    {
      "epoch": 1.84,
      "grad_norm": 1.163643717765808,
      "learning_rate": 0.000816,
      "loss": 0.72,
      "step": 46000
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.07819953560829163,
      "learning_rate": 0.0008139999999999999,
      "loss": 0.7134,
      "step": 46500
    },
    {
      "epoch": 1.88,
      "grad_norm": 587.955810546875,
      "learning_rate": 0.0008120000000000001,
      "loss": 0.7195,
      "step": 47000
    },
    {
      "epoch": 1.9,
      "grad_norm": 381.80517578125,
      "learning_rate": 0.0008100000000000001,
      "loss": 0.7537,
      "step": 47500
    },
    {
      "epoch": 1.92,
      "grad_norm": 144.209228515625,
      "learning_rate": 0.000808,
      "loss": 0.7228,
      "step": 48000
    },
    {
      "epoch": 1.94,
      "grad_norm": 86.45633697509766,
      "learning_rate": 0.0008060000000000001,
      "loss": 0.7481,
      "step": 48500
    },
    {
      "epoch": 1.96,
      "grad_norm": 106.03577423095703,
      "learning_rate": 0.000804,
      "loss": 0.7556,
      "step": 49000
    },
    {
      "epoch": 1.98,
      "grad_norm": 182.51702880859375,
      "learning_rate": 0.0008020000000000001,
      "loss": 0.7469,
      "step": 49500
    },
    {
      "epoch": 2.0,
      "grad_norm": 192.49668884277344,
      "learning_rate": 0.0008,
      "loss": 0.7631,
      "step": 50000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": {
        "accuracy": 0.85975
      },
      "eval_loss": 0.6808730363845825,
      "eval_runtime": 41.1541,
      "eval_samples_per_second": 485.979,
      "eval_steps_per_second": 121.495,
      "step": 50000
    },
    {
      "epoch": 2.02,
      "grad_norm": 1645.394287109375,
      "learning_rate": 0.0007980000000000001,
      "loss": 0.7506,
      "step": 50500
    },
    {
      "epoch": 2.04,
      "grad_norm": 8.931267738342285,
      "learning_rate": 0.000796,
      "loss": 0.7275,
      "step": 51000
    },
    {
      "epoch": 2.06,
      "grad_norm": 451.49090576171875,
      "learning_rate": 0.0007940000000000001,
      "loss": 0.7258,
      "step": 51500
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.08174752444028854,
      "learning_rate": 0.0007920000000000001,
      "loss": 0.7481,
      "step": 52000
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.3556652069091797,
      "learning_rate": 0.00079,
      "loss": 0.7553,
      "step": 52500
    },
    {
      "epoch": 2.12,
      "grad_norm": 14.897370338439941,
      "learning_rate": 0.0007880000000000001,
      "loss": 0.7493,
      "step": 53000
    },
    {
      "epoch": 2.14,
      "grad_norm": 24.16037368774414,
      "learning_rate": 0.000786,
      "loss": 0.7424,
      "step": 53500
    },
    {
      "epoch": 2.16,
      "grad_norm": 419.95281982421875,
      "learning_rate": 0.0007840000000000001,
      "loss": 0.7699,
      "step": 54000
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.9126428365707397,
      "learning_rate": 0.000782,
      "loss": 0.8107,
      "step": 54500
    },
    {
      "epoch": 2.2,
      "grad_norm": 256.14599609375,
      "learning_rate": 0.0007800000000000001,
      "loss": 0.8315,
      "step": 55000
    },
    {
      "epoch": 2.22,
      "grad_norm": 448.97357177734375,
      "learning_rate": 0.000778,
      "loss": 0.6788,
      "step": 55500
    },
    {
      "epoch": 2.24,
      "grad_norm": 504.53424072265625,
      "learning_rate": 0.000776,
      "loss": 0.7949,
      "step": 56000
    },
    {
      "epoch": 2.26,
      "grad_norm": 6.191288948059082,
      "learning_rate": 0.0007740000000000001,
      "loss": 0.7242,
      "step": 56500
    },
    {
      "epoch": 2.28,
      "grad_norm": 739.7684326171875,
      "learning_rate": 0.000772,
      "loss": 0.7363,
      "step": 57000
    },
    {
      "epoch": 2.3,
      "grad_norm": 207.5795135498047,
      "learning_rate": 0.0007700000000000001,
      "loss": 0.7325,
      "step": 57500
    },
    {
      "epoch": 2.32,
      "grad_norm": 269.013916015625,
      "learning_rate": 0.000768,
      "loss": 0.8053,
      "step": 58000
    },
    {
      "epoch": 2.34,
      "grad_norm": 347.04608154296875,
      "learning_rate": 0.0007660000000000001,
      "loss": 0.7366,
      "step": 58500
    },
    {
      "epoch": 2.36,
      "grad_norm": 305.58056640625,
      "learning_rate": 0.000764,
      "loss": 0.7879,
      "step": 59000
    },
    {
      "epoch": 2.38,
      "grad_norm": 8.59219741821289,
      "learning_rate": 0.000762,
      "loss": 0.7706,
      "step": 59500
    },
    {
      "epoch": 2.4,
      "grad_norm": 1093.40673828125,
      "learning_rate": 0.00076,
      "loss": 0.7055,
      "step": 60000
    },
    {
      "epoch": 2.42,
      "grad_norm": 55.57994842529297,
      "learning_rate": 0.000758,
      "loss": 0.7994,
      "step": 60500
    },
    {
      "epoch": 2.44,
      "grad_norm": 35.18922805786133,
      "learning_rate": 0.000756,
      "loss": 0.7563,
      "step": 61000
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.88185715675354,
      "learning_rate": 0.000754,
      "loss": 0.7345,
      "step": 61500
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.2963314056396484,
      "learning_rate": 0.0007520000000000001,
      "loss": 0.7071,
      "step": 62000
    },
    {
      "epoch": 2.5,
      "grad_norm": 140.44546508789062,
      "learning_rate": 0.00075,
      "loss": 0.7493,
      "step": 62500
    },
    {
      "epoch": 2.52,
      "grad_norm": 1078.35986328125,
      "learning_rate": 0.000748,
      "loss": 0.7612,
      "step": 63000
    },
    {
      "epoch": 2.54,
      "grad_norm": 120.49620056152344,
      "learning_rate": 0.000746,
      "loss": 0.7992,
      "step": 63500
    },
    {
      "epoch": 2.56,
      "grad_norm": 140.32275390625,
      "learning_rate": 0.000744,
      "loss": 0.7754,
      "step": 64000
    },
    {
      "epoch": 2.58,
      "grad_norm": 513.8121337890625,
      "learning_rate": 0.000742,
      "loss": 0.7154,
      "step": 64500
    },
    {
      "epoch": 2.6,
      "grad_norm": 445.6025390625,
      "learning_rate": 0.00074,
      "loss": 0.8199,
      "step": 65000
    },
    {
      "epoch": 2.62,
      "grad_norm": 557.657958984375,
      "learning_rate": 0.000738,
      "loss": 0.7269,
      "step": 65500
    },
    {
      "epoch": 2.64,
      "grad_norm": 530.6468505859375,
      "learning_rate": 0.000736,
      "loss": 0.7245,
      "step": 66000
    },
    {
      "epoch": 2.66,
      "grad_norm": 935.8909912109375,
      "learning_rate": 0.000734,
      "loss": 0.7417,
      "step": 66500
    },
    {
      "epoch": 2.68,
      "grad_norm": 353.7798767089844,
      "learning_rate": 0.000732,
      "loss": 0.7196,
      "step": 67000
    },
    {
      "epoch": 2.7,
      "grad_norm": 264.1173095703125,
      "learning_rate": 0.00073,
      "loss": 0.7522,
      "step": 67500
    },
    {
      "epoch": 2.72,
      "grad_norm": 465.19451904296875,
      "learning_rate": 0.000728,
      "loss": 0.8764,
      "step": 68000
    },
    {
      "epoch": 2.74,
      "grad_norm": 1252.6422119140625,
      "learning_rate": 0.000726,
      "loss": 0.8052,
      "step": 68500
    },
    {
      "epoch": 2.76,
      "grad_norm": 231.36427307128906,
      "learning_rate": 0.000724,
      "loss": 0.7091,
      "step": 69000
    },
    {
      "epoch": 2.78,
      "grad_norm": 1346.1572265625,
      "learning_rate": 0.000722,
      "loss": 0.7707,
      "step": 69500
    },
    {
      "epoch": 2.8,
      "grad_norm": 263.8692932128906,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.7446,
      "step": 70000
    },
    {
      "epoch": 2.82,
      "grad_norm": 496.4037170410156,
      "learning_rate": 0.000718,
      "loss": 0.8652,
      "step": 70500
    },
    {
      "epoch": 2.84,
      "grad_norm": 545.7474365234375,
      "learning_rate": 0.000716,
      "loss": 0.7936,
      "step": 71000
    },
    {
      "epoch": 2.86,
      "grad_norm": 408.9639892578125,
      "learning_rate": 0.000714,
      "loss": 0.8068,
      "step": 71500
    },
    {
      "epoch": 2.88,
      "grad_norm": 295.8630065917969,
      "learning_rate": 0.000712,
      "loss": 0.7689,
      "step": 72000
    },
    {
      "epoch": 2.9,
      "grad_norm": 12.97110652923584,
      "learning_rate": 0.00071,
      "loss": 0.753,
      "step": 72500
    },
    {
      "epoch": 2.92,
      "grad_norm": 68.56658935546875,
      "learning_rate": 0.000708,
      "loss": 0.791,
      "step": 73000
    },
    {
      "epoch": 2.94,
      "grad_norm": 247.6959228515625,
      "learning_rate": 0.0007059999999999999,
      "loss": 0.7494,
      "step": 73500
    },
    {
      "epoch": 2.96,
      "grad_norm": 446.04052734375,
      "learning_rate": 0.000704,
      "loss": 0.8186,
      "step": 74000
    },
    {
      "epoch": 2.98,
      "grad_norm": 689.493896484375,
      "learning_rate": 0.0007019999999999999,
      "loss": 0.7471,
      "step": 74500
    },
    {
      "epoch": 3.0,
      "grad_norm": 382.2821960449219,
      "learning_rate": 0.0007,
      "loss": 0.818,
      "step": 75000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": {
        "accuracy": 0.8479
      },
      "eval_loss": 0.7645283937454224,
      "eval_runtime": 38.3246,
      "eval_samples_per_second": 521.858,
      "eval_steps_per_second": 130.464,
      "step": 75000
    },
    {
      "epoch": 3.02,
      "grad_norm": 34.856170654296875,
      "learning_rate": 0.0006979999999999999,
      "loss": 0.7842,
      "step": 75500
    },
    {
      "epoch": 3.04,
      "grad_norm": 4.359329700469971,
      "learning_rate": 0.000696,
      "loss": 0.8019,
      "step": 76000
    },
    {
      "epoch": 3.06,
      "grad_norm": 438.27093505859375,
      "learning_rate": 0.000694,
      "loss": 0.7326,
      "step": 76500
    },
    {
      "epoch": 3.08,
      "grad_norm": 381.7764892578125,
      "learning_rate": 0.000692,
      "loss": 0.7519,
      "step": 77000
    },
    {
      "epoch": 3.1,
      "grad_norm": 1168.9764404296875,
      "learning_rate": 0.00069,
      "loss": 0.7562,
      "step": 77500
    },
    {
      "epoch": 3.12,
      "grad_norm": 284.5892333984375,
      "learning_rate": 0.0006879999999999999,
      "loss": 0.7774,
      "step": 78000
    },
    {
      "epoch": 3.14,
      "grad_norm": 303.24951171875,
      "learning_rate": 0.0006860000000000001,
      "loss": 0.79,
      "step": 78500
    },
    {
      "epoch": 3.16,
      "grad_norm": 176.5148468017578,
      "learning_rate": 0.000684,
      "loss": 0.727,
      "step": 79000
    },
    {
      "epoch": 3.18,
      "grad_norm": 324.7411193847656,
      "learning_rate": 0.0006820000000000001,
      "loss": 0.7849,
      "step": 79500
    },
    {
      "epoch": 3.2,
      "grad_norm": 102.39532470703125,
      "learning_rate": 0.00068,
      "loss": 0.7776,
      "step": 80000
    },
    {
      "epoch": 3.22,
      "grad_norm": 8.317693710327148,
      "learning_rate": 0.0006780000000000001,
      "loss": 0.7314,
      "step": 80500
    },
    {
      "epoch": 3.24,
      "grad_norm": 388.10980224609375,
      "learning_rate": 0.0006760000000000001,
      "loss": 0.6971,
      "step": 81000
    },
    {
      "epoch": 3.26,
      "grad_norm": 4.6065287590026855,
      "learning_rate": 0.000674,
      "loss": 0.7161,
      "step": 81500
    },
    {
      "epoch": 3.28,
      "grad_norm": 812.8987426757812,
      "learning_rate": 0.0006720000000000001,
      "loss": 0.6795,
      "step": 82000
    },
    {
      "epoch": 3.3,
      "grad_norm": 246.3987579345703,
      "learning_rate": 0.00067,
      "loss": 0.7829,
      "step": 82500
    },
    {
      "epoch": 3.32,
      "grad_norm": 118.4791488647461,
      "learning_rate": 0.0006680000000000001,
      "loss": 0.8019,
      "step": 83000
    },
    {
      "epoch": 3.34,
      "grad_norm": 357.0204772949219,
      "learning_rate": 0.000666,
      "loss": 0.6855,
      "step": 83500
    },
    {
      "epoch": 3.36,
      "grad_norm": 238.29905700683594,
      "learning_rate": 0.0006640000000000001,
      "loss": 0.7232,
      "step": 84000
    },
    {
      "epoch": 3.38,
      "grad_norm": 398.4237976074219,
      "learning_rate": 0.000662,
      "loss": 0.8271,
      "step": 84500
    },
    {
      "epoch": 3.4,
      "grad_norm": 849.763671875,
      "learning_rate": 0.00066,
      "loss": 0.8027,
      "step": 85000
    },
    {
      "epoch": 3.42,
      "grad_norm": 441.2332458496094,
      "learning_rate": 0.0006580000000000001,
      "loss": 0.7035,
      "step": 85500
    },
    {
      "epoch": 3.44,
      "grad_norm": 221.03411865234375,
      "learning_rate": 0.000656,
      "loss": 0.7321,
      "step": 86000
    },
    {
      "epoch": 3.46,
      "grad_norm": 280.0106506347656,
      "learning_rate": 0.0006540000000000001,
      "loss": 0.7403,
      "step": 86500
    },
    {
      "epoch": 3.48,
      "grad_norm": 251.7000274658203,
      "learning_rate": 0.000652,
      "loss": 0.753,
      "step": 87000
    },
    {
      "epoch": 3.5,
      "grad_norm": 546.8394775390625,
      "learning_rate": 0.0006500000000000001,
      "loss": 0.7532,
      "step": 87500
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.9923107624053955,
      "learning_rate": 0.000648,
      "loss": 0.7401,
      "step": 88000
    },
    {
      "epoch": 3.54,
      "grad_norm": 611.3046264648438,
      "learning_rate": 0.000646,
      "loss": 0.7251,
      "step": 88500
    },
    {
      "epoch": 3.56,
      "grad_norm": 8.793217658996582,
      "learning_rate": 0.000644,
      "loss": 0.7574,
      "step": 89000
    },
    {
      "epoch": 3.58,
      "grad_norm": 111.47941589355469,
      "learning_rate": 0.000642,
      "loss": 0.7741,
      "step": 89500
    },
    {
      "epoch": 3.6,
      "grad_norm": 1034.6622314453125,
      "learning_rate": 0.00064,
      "loss": 0.684,
      "step": 90000
    },
    {
      "epoch": 3.62,
      "grad_norm": 25.55658531188965,
      "learning_rate": 0.000638,
      "loss": 0.7265,
      "step": 90500
    },
    {
      "epoch": 3.64,
      "grad_norm": 33.427947998046875,
      "learning_rate": 0.0006360000000000001,
      "loss": 0.7243,
      "step": 91000
    },
    {
      "epoch": 3.66,
      "grad_norm": 440.321044921875,
      "learning_rate": 0.000634,
      "loss": 0.7562,
      "step": 91500
    },
    {
      "epoch": 3.68,
      "grad_norm": 81.01660919189453,
      "learning_rate": 0.000632,
      "loss": 0.7349,
      "step": 92000
    },
    {
      "epoch": 3.7,
      "grad_norm": 3.426272392272949,
      "learning_rate": 0.00063,
      "loss": 0.6962,
      "step": 92500
    },
    {
      "epoch": 3.72,
      "grad_norm": 2184.64306640625,
      "learning_rate": 0.000628,
      "loss": 0.7768,
      "step": 93000
    },
    {
      "epoch": 3.74,
      "grad_norm": 11.006987571716309,
      "learning_rate": 0.000626,
      "loss": 0.7663,
      "step": 93500
    },
    {
      "epoch": 3.76,
      "grad_norm": 2498.764404296875,
      "learning_rate": 0.000624,
      "loss": 0.7789,
      "step": 94000
    },
    {
      "epoch": 3.78,
      "grad_norm": 7.189889907836914,
      "learning_rate": 0.000622,
      "loss": 0.767,
      "step": 94500
    },
    {
      "epoch": 3.8,
      "grad_norm": 29.47304916381836,
      "learning_rate": 0.00062,
      "loss": 0.7728,
      "step": 95000
    },
    {
      "epoch": 3.82,
      "grad_norm": 302.2178649902344,
      "learning_rate": 0.0006180000000000001,
      "loss": 0.7322,
      "step": 95500
    },
    {
      "epoch": 3.84,
      "grad_norm": 42.226722717285156,
      "learning_rate": 0.000616,
      "loss": 0.775,
      "step": 96000
    },
    {
      "epoch": 3.86,
      "grad_norm": 196.9398956298828,
      "learning_rate": 0.000614,
      "loss": 0.7562,
      "step": 96500
    },
    {
      "epoch": 3.88,
      "grad_norm": 8.422393798828125,
      "learning_rate": 0.000612,
      "loss": 0.746,
      "step": 97000
    },
    {
      "epoch": 3.9,
      "grad_norm": 109.93981170654297,
      "learning_rate": 0.00061,
      "loss": 0.8217,
      "step": 97500
    },
    {
      "epoch": 3.92,
      "grad_norm": 1948.95703125,
      "learning_rate": 0.000608,
      "loss": 0.7565,
      "step": 98000
    },
    {
      "epoch": 3.94,
      "grad_norm": 12.216506958007812,
      "learning_rate": 0.000606,
      "loss": 0.8079,
      "step": 98500
    },
    {
      "epoch": 3.96,
      "grad_norm": 774.7198486328125,
      "learning_rate": 0.000604,
      "loss": 0.7068,
      "step": 99000
    },
    {
      "epoch": 3.98,
      "grad_norm": 808.7705078125,
      "learning_rate": 0.000602,
      "loss": 0.7438,
      "step": 99500
    },
    {
      "epoch": 4.0,
      "grad_norm": 698.9755859375,
      "learning_rate": 0.0006,
      "loss": 0.7534,
      "step": 100000
    },
    {
      "epoch": 4.0,
      "eval_accuracy": {
        "accuracy": 0.851
      },
      "eval_loss": 0.7133814692497253,
      "eval_runtime": 39.1272,
      "eval_samples_per_second": 511.154,
      "eval_steps_per_second": 127.788,
      "step": 100000
    },
    {
      "epoch": 4.02,
      "grad_norm": 448.591064453125,
      "learning_rate": 0.000598,
      "loss": 0.7741,
      "step": 100500
    },
    {
      "epoch": 4.04,
      "grad_norm": 152.9704132080078,
      "learning_rate": 0.000596,
      "loss": 0.7555,
      "step": 101000
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.9609742760658264,
      "learning_rate": 0.000594,
      "loss": 0.756,
      "step": 101500
    },
    {
      "epoch": 4.08,
      "grad_norm": 303.1271667480469,
      "learning_rate": 0.000592,
      "loss": 0.7839,
      "step": 102000
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.5992464423179626,
      "learning_rate": 0.00059,
      "loss": 0.7981,
      "step": 102500
    },
    {
      "epoch": 4.12,
      "grad_norm": 2287.62255859375,
      "learning_rate": 0.000588,
      "loss": 0.7129,
      "step": 103000
    },
    {
      "epoch": 4.14,
      "grad_norm": 50.36689758300781,
      "learning_rate": 0.0005859999999999999,
      "loss": 0.7197,
      "step": 103500
    },
    {
      "epoch": 4.16,
      "grad_norm": 1721.1329345703125,
      "learning_rate": 0.000584,
      "loss": 0.744,
      "step": 104000
    },
    {
      "epoch": 4.18,
      "grad_norm": 2.1559488773345947,
      "learning_rate": 0.0005819999999999999,
      "loss": 0.7316,
      "step": 104500
    },
    {
      "epoch": 4.2,
      "grad_norm": 261.7562255859375,
      "learning_rate": 0.00058,
      "loss": 0.7741,
      "step": 105000
    },
    {
      "epoch": 4.22,
      "grad_norm": 11.875345230102539,
      "learning_rate": 0.000578,
      "loss": 0.7544,
      "step": 105500
    },
    {
      "epoch": 4.24,
      "grad_norm": 163.13311767578125,
      "learning_rate": 0.000576,
      "loss": 0.6927,
      "step": 106000
    },
    {
      "epoch": 4.26,
      "grad_norm": 225.0519561767578,
      "learning_rate": 0.000574,
      "loss": 0.7465,
      "step": 106500
    },
    {
      "epoch": 4.28,
      "grad_norm": 80.15457153320312,
      "learning_rate": 0.0005719999999999999,
      "loss": 0.7448,
      "step": 107000
    },
    {
      "epoch": 4.3,
      "grad_norm": 2.617668390274048,
      "learning_rate": 0.00057,
      "loss": 0.8114,
      "step": 107500
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.6164417862892151,
      "learning_rate": 0.0005679999999999999,
      "loss": 0.7284,
      "step": 108000
    },
    {
      "epoch": 4.34,
      "grad_norm": 161.5952911376953,
      "learning_rate": 0.000566,
      "loss": 0.7912,
      "step": 108500
    },
    {
      "epoch": 4.36,
      "grad_norm": 326.0581359863281,
      "learning_rate": 0.0005639999999999999,
      "loss": 0.7448,
      "step": 109000
    },
    {
      "epoch": 4.38,
      "grad_norm": 251.7965545654297,
      "learning_rate": 0.0005620000000000001,
      "loss": 0.7521,
      "step": 109500
    },
    {
      "epoch": 4.4,
      "grad_norm": 360.9570007324219,
      "learning_rate": 0.0005600000000000001,
      "loss": 0.7126,
      "step": 110000
    },
    {
      "epoch": 4.42,
      "grad_norm": 1.7385978698730469,
      "learning_rate": 0.000558,
      "loss": 0.7032,
      "step": 110500
    },
    {
      "epoch": 4.44,
      "grad_norm": 186.65272521972656,
      "learning_rate": 0.0005560000000000001,
      "loss": 0.8403,
      "step": 111000
    },
    {
      "epoch": 4.46,
      "grad_norm": 460.61016845703125,
      "learning_rate": 0.000554,
      "loss": 0.7348,
      "step": 111500
    },
    {
      "epoch": 4.48,
      "grad_norm": 360.7991943359375,
      "learning_rate": 0.0005520000000000001,
      "loss": 0.7679,
      "step": 112000
    },
    {
      "epoch": 4.5,
      "grad_norm": 115.23887634277344,
      "learning_rate": 0.00055,
      "loss": 0.7393,
      "step": 112500
    },
    {
      "epoch": 4.52,
      "grad_norm": 182.8375244140625,
      "learning_rate": 0.0005480000000000001,
      "loss": 0.7418,
      "step": 113000
    },
    {
      "epoch": 4.54,
      "grad_norm": 37.827030181884766,
      "learning_rate": 0.000546,
      "loss": 0.8027,
      "step": 113500
    },
    {
      "epoch": 4.56,
      "grad_norm": 4.889217376708984,
      "learning_rate": 0.0005440000000000001,
      "loss": 0.7939,
      "step": 114000
    },
    {
      "epoch": 4.58,
      "grad_norm": 283.3753967285156,
      "learning_rate": 0.0005420000000000001,
      "loss": 0.7919,
      "step": 114500
    },
    {
      "epoch": 4.6,
      "grad_norm": 211.84072875976562,
      "learning_rate": 0.00054,
      "loss": 0.6549,
      "step": 115000
    },
    {
      "epoch": 4.62,
      "grad_norm": 687.2828369140625,
      "learning_rate": 0.0005380000000000001,
      "loss": 0.7242,
      "step": 115500
    },
    {
      "epoch": 4.64,
      "grad_norm": 923.770263671875,
      "learning_rate": 0.000536,
      "loss": 0.7052,
      "step": 116000
    },
    {
      "epoch": 4.66,
      "grad_norm": 153.45733642578125,
      "learning_rate": 0.0005340000000000001,
      "loss": 0.6557,
      "step": 116500
    },
    {
      "epoch": 4.68,
      "grad_norm": 5.791944980621338,
      "learning_rate": 0.000532,
      "loss": 0.6932,
      "step": 117000
    },
    {
      "epoch": 4.7,
      "grad_norm": 9.518250465393066,
      "learning_rate": 0.0005300000000000001,
      "loss": 0.7737,
      "step": 117500
    },
    {
      "epoch": 4.72,
      "grad_norm": 553.18798828125,
      "learning_rate": 0.000528,
      "loss": 0.7316,
      "step": 118000
    },
    {
      "epoch": 4.74,
      "grad_norm": 41.08219909667969,
      "learning_rate": 0.000526,
      "loss": 0.7244,
      "step": 118500
    },
    {
      "epoch": 4.76,
      "grad_norm": 114.85610961914062,
      "learning_rate": 0.000524,
      "loss": 0.7377,
      "step": 119000
    },
    {
      "epoch": 4.78,
      "grad_norm": 387.1578369140625,
      "learning_rate": 0.000522,
      "loss": 0.7125,
      "step": 119500
    },
    {
      "epoch": 4.8,
      "grad_norm": 150.4223175048828,
      "learning_rate": 0.0005200000000000001,
      "loss": 0.7348,
      "step": 120000
    },
    {
      "epoch": 4.82,
      "grad_norm": 6.009800434112549,
      "learning_rate": 0.000518,
      "loss": 0.615,
      "step": 120500
    },
    {
      "epoch": 4.84,
      "grad_norm": 391.11175537109375,
      "learning_rate": 0.0005160000000000001,
      "loss": 0.7527,
      "step": 121000
    },
    {
      "epoch": 4.86,
      "grad_norm": 27.40836524963379,
      "learning_rate": 0.000514,
      "loss": 0.7302,
      "step": 121500
    },
    {
      "epoch": 4.88,
      "grad_norm": 173.09837341308594,
      "learning_rate": 0.000512,
      "loss": 0.6845,
      "step": 122000
    },
    {
      "epoch": 4.9,
      "grad_norm": 4512.09765625,
      "learning_rate": 0.00051,
      "loss": 0.7153,
      "step": 122500
    },
    {
      "epoch": 4.92,
      "grad_norm": 12.741973876953125,
      "learning_rate": 0.000508,
      "loss": 0.7422,
      "step": 123000
    },
    {
      "epoch": 4.94,
      "grad_norm": 774.936279296875,
      "learning_rate": 0.000506,
      "loss": 0.6882,
      "step": 123500
    },
    {
      "epoch": 4.96,
      "grad_norm": 222.51087951660156,
      "learning_rate": 0.000504,
      "loss": 0.7313,
      "step": 124000
    },
    {
      "epoch": 4.98,
      "grad_norm": 189.41856384277344,
      "learning_rate": 0.0005020000000000001,
      "loss": 0.7356,
      "step": 124500
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.06422700732946396,
      "learning_rate": 0.0005,
      "loss": 0.7069,
      "step": 125000
    },
    {
      "epoch": 5.0,
      "eval_accuracy": {
        "accuracy": 0.85385
      },
      "eval_loss": 0.7499802708625793,
      "eval_runtime": 41.3601,
      "eval_samples_per_second": 483.557,
      "eval_steps_per_second": 120.889,
      "step": 125000
    },
    {
      "epoch": 5.02,
      "grad_norm": 316.57177734375,
      "learning_rate": 0.000498,
      "loss": 0.7387,
      "step": 125500
    },
    {
      "epoch": 5.04,
      "grad_norm": 42.03173828125,
      "learning_rate": 0.000496,
      "loss": 0.6844,
      "step": 126000
    },
    {
      "epoch": 5.06,
      "grad_norm": 72.66574096679688,
      "learning_rate": 0.000494,
      "loss": 0.733,
      "step": 126500
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.3442893624305725,
      "learning_rate": 0.000492,
      "loss": 0.6983,
      "step": 127000
    },
    {
      "epoch": 5.1,
      "grad_norm": 582.3744506835938,
      "learning_rate": 0.00049,
      "loss": 0.7644,
      "step": 127500
    },
    {
      "epoch": 5.12,
      "grad_norm": 302.3772888183594,
      "learning_rate": 0.000488,
      "loss": 0.7004,
      "step": 128000
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.10566936433315277,
      "learning_rate": 0.000486,
      "loss": 0.7017,
      "step": 128500
    },
    {
      "epoch": 5.16,
      "grad_norm": 2780.4296875,
      "learning_rate": 0.000484,
      "loss": 0.7474,
      "step": 129000
    },
    {
      "epoch": 5.18,
      "grad_norm": 13.419196128845215,
      "learning_rate": 0.000482,
      "loss": 0.7658,
      "step": 129500
    },
    {
      "epoch": 5.2,
      "grad_norm": 545.4178466796875,
      "learning_rate": 0.00048,
      "loss": 0.6418,
      "step": 130000
    },
    {
      "epoch": 5.22,
      "grad_norm": 73.84529113769531,
      "learning_rate": 0.00047799999999999996,
      "loss": 0.7659,
      "step": 130500
    },
    {
      "epoch": 5.24,
      "grad_norm": 145.0167694091797,
      "learning_rate": 0.00047599999999999997,
      "loss": 0.6615,
      "step": 131000
    },
    {
      "epoch": 5.26,
      "grad_norm": 75.46564483642578,
      "learning_rate": 0.000474,
      "loss": 0.7098,
      "step": 131500
    },
    {
      "epoch": 5.28,
      "grad_norm": 389.6264953613281,
      "learning_rate": 0.000472,
      "loss": 0.7268,
      "step": 132000
    },
    {
      "epoch": 5.3,
      "grad_norm": 167.6171875,
      "learning_rate": 0.00047,
      "loss": 0.7308,
      "step": 132500
    },
    {
      "epoch": 5.32,
      "grad_norm": 1.1777647733688354,
      "learning_rate": 0.00046800000000000005,
      "loss": 0.6993,
      "step": 133000
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.627841591835022,
      "learning_rate": 0.00046600000000000005,
      "loss": 0.7053,
      "step": 133500
    },
    {
      "epoch": 5.36,
      "grad_norm": 10.954928398132324,
      "learning_rate": 0.00046400000000000006,
      "loss": 0.7228,
      "step": 134000
    },
    {
      "epoch": 5.38,
      "grad_norm": 6.693686008453369,
      "learning_rate": 0.000462,
      "loss": 0.6623,
      "step": 134500
    },
    {
      "epoch": 5.4,
      "grad_norm": 213.9030303955078,
      "learning_rate": 0.00046,
      "loss": 0.708,
      "step": 135000
    },
    {
      "epoch": 5.42,
      "grad_norm": 371.24920654296875,
      "learning_rate": 0.000458,
      "loss": 0.7973,
      "step": 135500
    },
    {
      "epoch": 5.44,
      "grad_norm": 71.06494903564453,
      "learning_rate": 0.000456,
      "loss": 0.6969,
      "step": 136000
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.12395527958869934,
      "learning_rate": 0.00045400000000000003,
      "loss": 0.7677,
      "step": 136500
    },
    {
      "epoch": 5.48,
      "grad_norm": 224.33010864257812,
      "learning_rate": 0.00045200000000000004,
      "loss": 0.6891,
      "step": 137000
    },
    {
      "epoch": 5.5,
      "grad_norm": 115.3927001953125,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.7207,
      "step": 137500
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.27870404720306396,
      "learning_rate": 0.000448,
      "loss": 0.67,
      "step": 138000
    },
    {
      "epoch": 5.54,
      "grad_norm": 755.419189453125,
      "learning_rate": 0.000446,
      "loss": 0.7581,
      "step": 138500
    },
    {
      "epoch": 5.56,
      "grad_norm": 902.3057861328125,
      "learning_rate": 0.000444,
      "loss": 0.7252,
      "step": 139000
    },
    {
      "epoch": 5.58,
      "grad_norm": 23.86826515197754,
      "learning_rate": 0.000442,
      "loss": 0.7056,
      "step": 139500
    },
    {
      "epoch": 5.6,
      "grad_norm": 241.62547302246094,
      "learning_rate": 0.00044,
      "loss": 0.8001,
      "step": 140000
    },
    {
      "epoch": 5.62,
      "grad_norm": 137.78839111328125,
      "learning_rate": 0.000438,
      "loss": 0.7488,
      "step": 140500
    },
    {
      "epoch": 5.64,
      "grad_norm": 259.2609558105469,
      "learning_rate": 0.000436,
      "loss": 0.7228,
      "step": 141000
    },
    {
      "epoch": 5.66,
      "grad_norm": 515.2636108398438,
      "learning_rate": 0.00043400000000000003,
      "loss": 0.7239,
      "step": 141500
    },
    {
      "epoch": 5.68,
      "grad_norm": 96.66551208496094,
      "learning_rate": 0.000432,
      "loss": 0.6939,
      "step": 142000
    },
    {
      "epoch": 5.7,
      "grad_norm": 491.6911315917969,
      "learning_rate": 0.00043,
      "loss": 0.724,
      "step": 142500
    },
    {
      "epoch": 5.72,
      "grad_norm": 3.3586432933807373,
      "learning_rate": 0.000428,
      "loss": 0.7529,
      "step": 143000
    },
    {
      "epoch": 5.74,
      "grad_norm": 161.73057556152344,
      "learning_rate": 0.000426,
      "loss": 0.7004,
      "step": 143500
    },
    {
      "epoch": 5.76,
      "grad_norm": 2.058309555053711,
      "learning_rate": 0.000424,
      "loss": 0.7326,
      "step": 144000
    },
    {
      "epoch": 5.78,
      "grad_norm": 217.74794006347656,
      "learning_rate": 0.000422,
      "loss": 0.6995,
      "step": 144500
    },
    {
      "epoch": 5.8,
      "grad_norm": 1673.010498046875,
      "learning_rate": 0.00042,
      "loss": 0.6807,
      "step": 145000
    },
    {
      "epoch": 5.82,
      "grad_norm": 62.84111785888672,
      "learning_rate": 0.00041799999999999997,
      "loss": 0.7123,
      "step": 145500
    },
    {
      "epoch": 5.84,
      "grad_norm": 230.7152557373047,
      "learning_rate": 0.000416,
      "loss": 0.712,
      "step": 146000
    },
    {
      "epoch": 5.86,
      "grad_norm": 246.16249084472656,
      "learning_rate": 0.000414,
      "loss": 0.6983,
      "step": 146500
    },
    {
      "epoch": 5.88,
      "grad_norm": 7.739560127258301,
      "learning_rate": 0.000412,
      "loss": 0.6816,
      "step": 147000
    },
    {
      "epoch": 5.9,
      "grad_norm": 465.3836364746094,
      "learning_rate": 0.00041,
      "loss": 0.731,
      "step": 147500
    },
    {
      "epoch": 5.92,
      "grad_norm": 159.96543884277344,
      "learning_rate": 0.000408,
      "loss": 0.8047,
      "step": 148000
    },
    {
      "epoch": 5.94,
      "grad_norm": 312.50732421875,
      "learning_rate": 0.00040600000000000006,
      "loss": 0.6853,
      "step": 148500
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.4150084853172302,
      "learning_rate": 0.000404,
      "loss": 0.7287,
      "step": 149000
    },
    {
      "epoch": 5.98,
      "grad_norm": 227.0913848876953,
      "learning_rate": 0.000402,
      "loss": 0.7029,
      "step": 149500
    },
    {
      "epoch": 6.0,
      "grad_norm": 361.9114074707031,
      "learning_rate": 0.0004,
      "loss": 0.8364,
      "step": 150000
    },
    {
      "epoch": 6.0,
      "eval_accuracy": {
        "accuracy": 0.8563
      },
      "eval_loss": 0.7079243659973145,
      "eval_runtime": 45.1663,
      "eval_samples_per_second": 442.808,
      "eval_steps_per_second": 110.702,
      "step": 150000
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.7918673753738403,
      "learning_rate": 0.000398,
      "loss": 0.6928,
      "step": 150500
    },
    {
      "epoch": 6.04,
      "grad_norm": 14.788847923278809,
      "learning_rate": 0.00039600000000000003,
      "loss": 0.6597,
      "step": 151000
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.15075013041496277,
      "learning_rate": 0.00039400000000000004,
      "loss": 0.7583,
      "step": 151500
    },
    {
      "epoch": 6.08,
      "grad_norm": 889.8311767578125,
      "learning_rate": 0.00039200000000000004,
      "loss": 0.7545,
      "step": 152000
    },
    {
      "epoch": 6.1,
      "grad_norm": 285.4797058105469,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.7066,
      "step": 152500
    },
    {
      "epoch": 6.12,
      "grad_norm": 457.24017333984375,
      "learning_rate": 0.000388,
      "loss": 0.8217,
      "step": 153000
    },
    {
      "epoch": 6.14,
      "grad_norm": 4.580352306365967,
      "learning_rate": 0.000386,
      "loss": 0.7094,
      "step": 153500
    },
    {
      "epoch": 6.16,
      "grad_norm": 64.83468627929688,
      "learning_rate": 0.000384,
      "loss": 0.6629,
      "step": 154000
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.0032391175627708435,
      "learning_rate": 0.000382,
      "loss": 0.6874,
      "step": 154500
    },
    {
      "epoch": 6.2,
      "grad_norm": 4.07515287399292,
      "learning_rate": 0.00038,
      "loss": 0.6867,
      "step": 155000
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.6392061710357666,
      "learning_rate": 0.000378,
      "loss": 0.7703,
      "step": 155500
    },
    {
      "epoch": 6.24,
      "grad_norm": 3820.865966796875,
      "learning_rate": 0.00037600000000000003,
      "loss": 0.7037,
      "step": 156000
    },
    {
      "epoch": 6.26,
      "grad_norm": 203.9265594482422,
      "learning_rate": 0.000374,
      "loss": 0.7412,
      "step": 156500
    },
    {
      "epoch": 6.28,
      "grad_norm": 496.9651184082031,
      "learning_rate": 0.000372,
      "loss": 0.7106,
      "step": 157000
    },
    {
      "epoch": 6.3,
      "grad_norm": 24.886266708374023,
      "learning_rate": 0.00037,
      "loss": 0.7276,
      "step": 157500
    },
    {
      "epoch": 6.32,
      "grad_norm": 87.77192687988281,
      "learning_rate": 0.000368,
      "loss": 0.7537,
      "step": 158000
    },
    {
      "epoch": 6.34,
      "grad_norm": 696.9080810546875,
      "learning_rate": 0.000366,
      "loss": 0.6672,
      "step": 158500
    },
    {
      "epoch": 6.36,
      "grad_norm": 270.9795227050781,
      "learning_rate": 0.000364,
      "loss": 0.7503,
      "step": 159000
    },
    {
      "epoch": 6.38,
      "grad_norm": 133.46510314941406,
      "learning_rate": 0.000362,
      "loss": 0.7557,
      "step": 159500
    },
    {
      "epoch": 6.4,
      "grad_norm": 517.9771728515625,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.5935,
      "step": 160000
    },
    {
      "epoch": 6.42,
      "grad_norm": 32.87696075439453,
      "learning_rate": 0.000358,
      "loss": 0.6611,
      "step": 160500
    },
    {
      "epoch": 6.44,
      "grad_norm": 35.35725402832031,
      "learning_rate": 0.000356,
      "loss": 0.7342,
      "step": 161000
    },
    {
      "epoch": 6.46,
      "grad_norm": 63.13096618652344,
      "learning_rate": 0.000354,
      "loss": 0.6885,
      "step": 161500
    },
    {
      "epoch": 6.48,
      "grad_norm": 363.7257080078125,
      "learning_rate": 0.000352,
      "loss": 0.6945,
      "step": 162000
    },
    {
      "epoch": 6.5,
      "grad_norm": 520.552001953125,
      "learning_rate": 0.00035,
      "loss": 0.7431,
      "step": 162500
    },
    {
      "epoch": 6.52,
      "grad_norm": 78.07764434814453,
      "learning_rate": 0.000348,
      "loss": 0.6516,
      "step": 163000
    },
    {
      "epoch": 6.54,
      "grad_norm": 198.06028747558594,
      "learning_rate": 0.000346,
      "loss": 0.6459,
      "step": 163500
    },
    {
      "epoch": 6.56,
      "grad_norm": 31.00834846496582,
      "learning_rate": 0.00034399999999999996,
      "loss": 0.7326,
      "step": 164000
    },
    {
      "epoch": 6.58,
      "grad_norm": 434.5670471191406,
      "learning_rate": 0.000342,
      "loss": 0.6547,
      "step": 164500
    },
    {
      "epoch": 6.6,
      "grad_norm": 10.540486335754395,
      "learning_rate": 0.00034,
      "loss": 0.7301,
      "step": 165000
    },
    {
      "epoch": 6.62,
      "grad_norm": 63.628173828125,
      "learning_rate": 0.00033800000000000003,
      "loss": 0.6687,
      "step": 165500
    },
    {
      "epoch": 6.64,
      "grad_norm": 570.628662109375,
      "learning_rate": 0.00033600000000000004,
      "loss": 0.6698,
      "step": 166000
    },
    {
      "epoch": 6.66,
      "grad_norm": 99.6751480102539,
      "learning_rate": 0.00033400000000000004,
      "loss": 0.6979,
      "step": 166500
    },
    {
      "epoch": 6.68,
      "grad_norm": 3.797018051147461,
      "learning_rate": 0.00033200000000000005,
      "loss": 0.6812,
      "step": 167000
    },
    {
      "epoch": 6.7,
      "grad_norm": 4.327022552490234,
      "learning_rate": 0.00033,
      "loss": 0.7237,
      "step": 167500
    },
    {
      "epoch": 6.72,
      "grad_norm": 191.68402099609375,
      "learning_rate": 0.000328,
      "loss": 0.6968,
      "step": 168000
    },
    {
      "epoch": 6.74,
      "grad_norm": 408.3288269042969,
      "learning_rate": 0.000326,
      "loss": 0.6889,
      "step": 168500
    },
    {
      "epoch": 6.76,
      "grad_norm": 261.3753662109375,
      "learning_rate": 0.000324,
      "loss": 0.7401,
      "step": 169000
    },
    {
      "epoch": 6.78,
      "grad_norm": 146.00123596191406,
      "learning_rate": 0.000322,
      "loss": 0.7128,
      "step": 169500
    },
    {
      "epoch": 6.8,
      "grad_norm": 278.7897033691406,
      "learning_rate": 0.00032,
      "loss": 0.6333,
      "step": 170000
    },
    {
      "epoch": 6.82,
      "grad_norm": 4.818150043487549,
      "learning_rate": 0.00031800000000000003,
      "loss": 0.7173,
      "step": 170500
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.22275958955287933,
      "learning_rate": 0.000316,
      "loss": 0.6988,
      "step": 171000
    },
    {
      "epoch": 6.86,
      "grad_norm": 425.2899169921875,
      "learning_rate": 0.000314,
      "loss": 0.7093,
      "step": 171500
    },
    {
      "epoch": 6.88,
      "grad_norm": 5.7640485763549805,
      "learning_rate": 0.000312,
      "loss": 0.6244,
      "step": 172000
    },
    {
      "epoch": 6.9,
      "grad_norm": 5.5073628425598145,
      "learning_rate": 0.00031,
      "loss": 0.651,
      "step": 172500
    },
    {
      "epoch": 6.92,
      "grad_norm": 9.499191284179688,
      "learning_rate": 0.000308,
      "loss": 0.698,
      "step": 173000
    },
    {
      "epoch": 6.94,
      "grad_norm": 127.8597412109375,
      "learning_rate": 0.000306,
      "loss": 0.6902,
      "step": 173500
    },
    {
      "epoch": 6.96,
      "grad_norm": 208.2137451171875,
      "learning_rate": 0.000304,
      "loss": 0.6582,
      "step": 174000
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.6926290392875671,
      "learning_rate": 0.000302,
      "loss": 0.6836,
      "step": 174500
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.18691499531269073,
      "learning_rate": 0.0003,
      "loss": 0.6864,
      "step": 175000
    },
    {
      "epoch": 7.0,
      "eval_accuracy": {
        "accuracy": 0.8631
      },
      "eval_loss": 0.673599123954773,
      "eval_runtime": 39.1371,
      "eval_samples_per_second": 511.024,
      "eval_steps_per_second": 127.756,
      "step": 175000
    },
    {
      "epoch": 7.02,
      "grad_norm": 306.4239501953125,
      "learning_rate": 0.000298,
      "loss": 0.6582,
      "step": 175500
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.29596027731895447,
      "learning_rate": 0.000296,
      "loss": 0.6984,
      "step": 176000
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.3888782560825348,
      "learning_rate": 0.000294,
      "loss": 0.6937,
      "step": 176500
    },
    {
      "epoch": 7.08,
      "grad_norm": 83.31741333007812,
      "learning_rate": 0.000292,
      "loss": 0.6775,
      "step": 177000
    },
    {
      "epoch": 7.1,
      "grad_norm": 82.25119018554688,
      "learning_rate": 0.00029,
      "loss": 0.6268,
      "step": 177500
    },
    {
      "epoch": 7.12,
      "grad_norm": 39.600101470947266,
      "learning_rate": 0.000288,
      "loss": 0.7463,
      "step": 178000
    },
    {
      "epoch": 7.14,
      "grad_norm": 2.8159360885620117,
      "learning_rate": 0.00028599999999999996,
      "loss": 0.6193,
      "step": 178500
    },
    {
      "epoch": 7.16,
      "grad_norm": 1.1956138610839844,
      "learning_rate": 0.00028399999999999996,
      "loss": 0.6621,
      "step": 179000
    },
    {
      "epoch": 7.18,
      "grad_norm": 230.99012756347656,
      "learning_rate": 0.00028199999999999997,
      "loss": 0.6615,
      "step": 179500
    },
    {
      "epoch": 7.2,
      "grad_norm": 1.5530050992965698,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.6245,
      "step": 180000
    },
    {
      "epoch": 7.22,
      "grad_norm": 1582.8348388671875,
      "learning_rate": 0.00027800000000000004,
      "loss": 0.6346,
      "step": 180500
    },
    {
      "epoch": 7.24,
      "grad_norm": 346.47625732421875,
      "learning_rate": 0.00027600000000000004,
      "loss": 0.6338,
      "step": 181000
    },
    {
      "epoch": 7.26,
      "grad_norm": 185.75315856933594,
      "learning_rate": 0.00027400000000000005,
      "loss": 0.699,
      "step": 181500
    },
    {
      "epoch": 7.28,
      "grad_norm": 1.8710582256317139,
      "learning_rate": 0.00027200000000000005,
      "loss": 0.5742,
      "step": 182000
    },
    {
      "epoch": 7.3,
      "grad_norm": 314.10675048828125,
      "learning_rate": 0.00027,
      "loss": 0.7012,
      "step": 182500
    },
    {
      "epoch": 7.32,
      "grad_norm": 98.61412048339844,
      "learning_rate": 0.000268,
      "loss": 0.6292,
      "step": 183000
    },
    {
      "epoch": 7.34,
      "grad_norm": 256.9787292480469,
      "learning_rate": 0.000266,
      "loss": 0.7418,
      "step": 183500
    },
    {
      "epoch": 7.36,
      "grad_norm": 29.305923461914062,
      "learning_rate": 0.000264,
      "loss": 0.6071,
      "step": 184000
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.045291718095541,
      "learning_rate": 0.000262,
      "loss": 0.6794,
      "step": 184500
    },
    {
      "epoch": 7.4,
      "grad_norm": 161.1195068359375,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.6658,
      "step": 185000
    },
    {
      "epoch": 7.42,
      "grad_norm": 27.628904342651367,
      "learning_rate": 0.00025800000000000004,
      "loss": 0.6231,
      "step": 185500
    },
    {
      "epoch": 7.44,
      "grad_norm": 3.441471576690674,
      "learning_rate": 0.000256,
      "loss": 0.69,
      "step": 186000
    },
    {
      "epoch": 7.46,
      "grad_norm": 3.4982025623321533,
      "learning_rate": 0.000254,
      "loss": 0.6023,
      "step": 186500
    },
    {
      "epoch": 7.48,
      "grad_norm": 1.5027453899383545,
      "learning_rate": 0.000252,
      "loss": 0.6879,
      "step": 187000
    },
    {
      "epoch": 7.5,
      "grad_norm": 284.7523193359375,
      "learning_rate": 0.00025,
      "loss": 0.6534,
      "step": 187500
    },
    {
      "epoch": 7.52,
      "grad_norm": 237.9163818359375,
      "learning_rate": 0.000248,
      "loss": 0.6495,
      "step": 188000
    },
    {
      "epoch": 7.54,
      "grad_norm": 0.2507583498954773,
      "learning_rate": 0.000246,
      "loss": 0.6871,
      "step": 188500
    },
    {
      "epoch": 7.56,
      "grad_norm": 383.24591064453125,
      "learning_rate": 0.000244,
      "loss": 0.5982,
      "step": 189000
    },
    {
      "epoch": 7.58,
      "grad_norm": 0.14287437498569489,
      "learning_rate": 0.000242,
      "loss": 0.5896,
      "step": 189500
    },
    {
      "epoch": 7.6,
      "grad_norm": 260.865966796875,
      "learning_rate": 0.00024,
      "loss": 0.6397,
      "step": 190000
    },
    {
      "epoch": 7.62,
      "grad_norm": 3.769435405731201,
      "learning_rate": 0.00023799999999999998,
      "loss": 0.7164,
      "step": 190500
    },
    {
      "epoch": 7.64,
      "grad_norm": 206.60784912109375,
      "learning_rate": 0.000236,
      "loss": 0.631,
      "step": 191000
    },
    {
      "epoch": 7.66,
      "grad_norm": 158.23878479003906,
      "learning_rate": 0.00023400000000000002,
      "loss": 0.5782,
      "step": 191500
    },
    {
      "epoch": 7.68,
      "grad_norm": 29.204374313354492,
      "learning_rate": 0.00023200000000000003,
      "loss": 0.61,
      "step": 192000
    },
    {
      "epoch": 7.7,
      "grad_norm": 6.1434645652771,
      "learning_rate": 0.00023,
      "loss": 0.5928,
      "step": 192500
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.11794833838939667,
      "learning_rate": 0.000228,
      "loss": 0.6362,
      "step": 193000
    },
    {
      "epoch": 7.74,
      "grad_norm": 1.4217901229858398,
      "learning_rate": 0.00022600000000000002,
      "loss": 0.666,
      "step": 193500
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.14166250824928284,
      "learning_rate": 0.000224,
      "loss": 0.5843,
      "step": 194000
    },
    {
      "epoch": 7.78,
      "grad_norm": 189.287841796875,
      "learning_rate": 0.000222,
      "loss": 0.628,
      "step": 194500
    },
    {
      "epoch": 7.8,
      "grad_norm": 1.225961685180664,
      "learning_rate": 0.00022,
      "loss": 0.6547,
      "step": 195000
    },
    {
      "epoch": 7.82,
      "grad_norm": 16.57257843017578,
      "learning_rate": 0.000218,
      "loss": 0.6331,
      "step": 195500
    },
    {
      "epoch": 7.84,
      "grad_norm": 44.91372299194336,
      "learning_rate": 0.000216,
      "loss": 0.6192,
      "step": 196000
    },
    {
      "epoch": 7.86,
      "grad_norm": 0.5365906357765198,
      "learning_rate": 0.000214,
      "loss": 0.6782,
      "step": 196500
    },
    {
      "epoch": 7.88,
      "grad_norm": 1.346395492553711,
      "learning_rate": 0.000212,
      "loss": 0.6507,
      "step": 197000
    },
    {
      "epoch": 7.9,
      "grad_norm": 0.4340331554412842,
      "learning_rate": 0.00021,
      "loss": 0.6068,
      "step": 197500
    },
    {
      "epoch": 7.92,
      "grad_norm": 93.64032745361328,
      "learning_rate": 0.000208,
      "loss": 0.6262,
      "step": 198000
    },
    {
      "epoch": 7.94,
      "grad_norm": 0.11091998219490051,
      "learning_rate": 0.000206,
      "loss": 0.5801,
      "step": 198500
    },
    {
      "epoch": 7.96,
      "grad_norm": 285.77862548828125,
      "learning_rate": 0.000204,
      "loss": 0.5854,
      "step": 199000
    },
    {
      "epoch": 7.98,
      "grad_norm": 522.0193481445312,
      "learning_rate": 0.000202,
      "loss": 0.5904,
      "step": 199500
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.10155043005943298,
      "learning_rate": 0.0002,
      "loss": 0.6086,
      "step": 200000
    },
    {
      "epoch": 8.0,
      "eval_accuracy": {
        "accuracy": 0.87525
      },
      "eval_loss": 0.6329543590545654,
      "eval_runtime": 53.8415,
      "eval_samples_per_second": 371.461,
      "eval_steps_per_second": 92.865,
      "step": 200000
    },
    {
      "epoch": 8.02,
      "grad_norm": 77.5625228881836,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.6388,
      "step": 200500
    },
    {
      "epoch": 8.04,
      "grad_norm": 72.93716430664062,
      "learning_rate": 0.00019600000000000002,
      "loss": 0.6716,
      "step": 201000
    },
    {
      "epoch": 8.06,
      "grad_norm": 197.79953002929688,
      "learning_rate": 0.000194,
      "loss": 0.626,
      "step": 201500
    },
    {
      "epoch": 8.08,
      "grad_norm": 112.9701919555664,
      "learning_rate": 0.000192,
      "loss": 0.637,
      "step": 202000
    },
    {
      "epoch": 8.1,
      "grad_norm": 5.647861957550049,
      "learning_rate": 0.00019,
      "loss": 0.6336,
      "step": 202500
    },
    {
      "epoch": 8.12,
      "grad_norm": 228.5606231689453,
      "learning_rate": 0.00018800000000000002,
      "loss": 0.616,
      "step": 203000
    },
    {
      "epoch": 8.14,
      "grad_norm": 145.81590270996094,
      "learning_rate": 0.000186,
      "loss": 0.6487,
      "step": 203500
    },
    {
      "epoch": 8.16,
      "grad_norm": 0.21272286772727966,
      "learning_rate": 0.000184,
      "loss": 0.6061,
      "step": 204000
    },
    {
      "epoch": 8.18,
      "grad_norm": 266.8478088378906,
      "learning_rate": 0.000182,
      "loss": 0.6231,
      "step": 204500
    },
    {
      "epoch": 8.2,
      "grad_norm": 389.45880126953125,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.5979,
      "step": 205000
    },
    {
      "epoch": 8.22,
      "grad_norm": 20.22481918334961,
      "learning_rate": 0.000178,
      "loss": 0.5903,
      "step": 205500
    },
    {
      "epoch": 8.24,
      "grad_norm": 108.20809936523438,
      "learning_rate": 0.000176,
      "loss": 0.5224,
      "step": 206000
    },
    {
      "epoch": 8.26,
      "grad_norm": 100.92646026611328,
      "learning_rate": 0.000174,
      "loss": 0.555,
      "step": 206500
    },
    {
      "epoch": 8.28,
      "grad_norm": 125.70943450927734,
      "learning_rate": 0.00017199999999999998,
      "loss": 0.6152,
      "step": 207000
    },
    {
      "epoch": 8.3,
      "grad_norm": 253.2769317626953,
      "learning_rate": 0.00017,
      "loss": 0.6229,
      "step": 207500
    },
    {
      "epoch": 8.32,
      "grad_norm": 420.7872314453125,
      "learning_rate": 0.00016800000000000002,
      "loss": 0.5801,
      "step": 208000
    },
    {
      "epoch": 8.34,
      "grad_norm": 0.24041663110256195,
      "learning_rate": 0.00016600000000000002,
      "loss": 0.6558,
      "step": 208500
    },
    {
      "epoch": 8.36,
      "grad_norm": 262.6770935058594,
      "learning_rate": 0.000164,
      "loss": 0.5748,
      "step": 209000
    },
    {
      "epoch": 8.38,
      "grad_norm": 0.8057581782341003,
      "learning_rate": 0.000162,
      "loss": 0.5761,
      "step": 209500
    },
    {
      "epoch": 8.4,
      "grad_norm": 223.75729370117188,
      "learning_rate": 0.00016,
      "loss": 0.6302,
      "step": 210000
    },
    {
      "epoch": 8.42,
      "grad_norm": 49.48249435424805,
      "learning_rate": 0.000158,
      "loss": 0.7189,
      "step": 210500
    },
    {
      "epoch": 8.44,
      "grad_norm": 850.9794921875,
      "learning_rate": 0.000156,
      "loss": 0.5046,
      "step": 211000
    },
    {
      "epoch": 8.46,
      "grad_norm": 0.043499067425727844,
      "learning_rate": 0.000154,
      "loss": 0.6254,
      "step": 211500
    },
    {
      "epoch": 8.48,
      "grad_norm": 14.32633113861084,
      "learning_rate": 0.000152,
      "loss": 0.6195,
      "step": 212000
    },
    {
      "epoch": 8.5,
      "grad_norm": 233.0404052734375,
      "learning_rate": 0.00015,
      "loss": 0.605,
      "step": 212500
    },
    {
      "epoch": 8.52,
      "grad_norm": 148.3554229736328,
      "learning_rate": 0.000148,
      "loss": 0.6201,
      "step": 213000
    },
    {
      "epoch": 8.54,
      "grad_norm": 0.11578372120857239,
      "learning_rate": 0.000146,
      "loss": 0.6121,
      "step": 213500
    },
    {
      "epoch": 8.56,
      "grad_norm": 97.8633041381836,
      "learning_rate": 0.000144,
      "loss": 0.6807,
      "step": 214000
    },
    {
      "epoch": 8.58,
      "grad_norm": 168.3057861328125,
      "learning_rate": 0.00014199999999999998,
      "loss": 0.6156,
      "step": 214500
    },
    {
      "epoch": 8.6,
      "grad_norm": 0.13913734257221222,
      "learning_rate": 0.00014000000000000001,
      "loss": 0.6191,
      "step": 215000
    },
    {
      "epoch": 8.62,
      "grad_norm": 233.3334503173828,
      "learning_rate": 0.00013800000000000002,
      "loss": 0.5551,
      "step": 215500
    },
    {
      "epoch": 8.64,
      "grad_norm": 255.89035034179688,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.6217,
      "step": 216000
    },
    {
      "epoch": 8.66,
      "grad_norm": 3.8958020210266113,
      "learning_rate": 0.000134,
      "loss": 0.6213,
      "step": 216500
    },
    {
      "epoch": 8.68,
      "grad_norm": 77.36711120605469,
      "learning_rate": 0.000132,
      "loss": 0.6013,
      "step": 217000
    },
    {
      "epoch": 8.7,
      "grad_norm": 173.50775146484375,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.6488,
      "step": 217500
    },
    {
      "epoch": 8.72,
      "grad_norm": 239.9093017578125,
      "learning_rate": 0.000128,
      "loss": 0.5643,
      "step": 218000
    },
    {
      "epoch": 8.74,
      "grad_norm": 20.40184211730957,
      "learning_rate": 0.000126,
      "loss": 0.5304,
      "step": 218500
    },
    {
      "epoch": 8.76,
      "grad_norm": 75.59432983398438,
      "learning_rate": 0.000124,
      "loss": 0.574,
      "step": 219000
    },
    {
      "epoch": 8.78,
      "grad_norm": 654.9869995117188,
      "learning_rate": 0.000122,
      "loss": 0.6259,
      "step": 219500
    },
    {
      "epoch": 8.8,
      "grad_norm": 7.371151924133301,
      "learning_rate": 0.00012,
      "loss": 0.5452,
      "step": 220000
    },
    {
      "epoch": 8.82,
      "grad_norm": 199.54629516601562,
      "learning_rate": 0.000118,
      "loss": 0.6591,
      "step": 220500
    },
    {
      "epoch": 8.84,
      "grad_norm": 231.44061279296875,
      "learning_rate": 0.00011600000000000001,
      "loss": 0.5539,
      "step": 221000
    },
    {
      "epoch": 8.86,
      "grad_norm": 168.5528564453125,
      "learning_rate": 0.000114,
      "loss": 0.5697,
      "step": 221500
    },
    {
      "epoch": 8.88,
      "grad_norm": 394.9910583496094,
      "learning_rate": 0.000112,
      "loss": 0.6393,
      "step": 222000
    },
    {
      "epoch": 8.9,
      "grad_norm": 10.007037162780762,
      "learning_rate": 0.00011,
      "loss": 0.6368,
      "step": 222500
    },
    {
      "epoch": 8.92,
      "grad_norm": 594.1217651367188,
      "learning_rate": 0.000108,
      "loss": 0.6267,
      "step": 223000
    },
    {
      "epoch": 8.94,
      "grad_norm": 612.7937622070312,
      "learning_rate": 0.000106,
      "loss": 0.5826,
      "step": 223500
    },
    {
      "epoch": 8.96,
      "grad_norm": 0.07806746661663055,
      "learning_rate": 0.000104,
      "loss": 0.5927,
      "step": 224000
    },
    {
      "epoch": 8.98,
      "grad_norm": 0.013277056626975536,
      "learning_rate": 0.000102,
      "loss": 0.6543,
      "step": 224500
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.03689083084464073,
      "learning_rate": 0.0001,
      "loss": 0.5881,
      "step": 225000
    },
    {
      "epoch": 9.0,
      "eval_accuracy": {
        "accuracy": 0.8831
      },
      "eval_loss": 0.5612757802009583,
      "eval_runtime": 37.3885,
      "eval_samples_per_second": 534.924,
      "eval_steps_per_second": 133.731,
      "step": 225000
    }
  ],
  "logging_steps": 500,
  "max_steps": 250000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "total_flos": 1.6871251918089408e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
